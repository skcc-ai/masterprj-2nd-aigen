"""
StructSynth Agent - ì½”ë“œ êµ¬ì¡° ë¶„ì„ ë° AST ì¶”ì¶œ
ì½”ë“œë² ì´ìŠ¤ì˜ êµ¬ì¡°ì  ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
"""

import os
import logging
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

from .parser import CodeParser
from .chunker import CodeChunker
from .llm_analyzer import LLMAnalyzer
from common.store.sqlite_store import SQLiteStore
from common.store.faiss_store import FAISSStore

logger = logging.getLogger(__name__)

class StructSynthAgent:
    """ì½”ë“œ êµ¬ì¡° ë¶„ì„ ë° AST ì¶”ì¶œ ì—ì´ì „íŠ¸"""
    
    def __init__(self, repo_path: str, artifacts_dir: str = "./artifacts", data_dir: str = "./data"):
        """
        StructSynthAgent ì´ˆê¸°í™”
        
        Args:
            repo_path: ë¶„ì„í•  ì €ì¥ì†Œ ê²½ë¡œ
            artifacts_dir: ê²°ê³¼ë¬¼ ì €ì¥ ë””ë ‰í† ë¦¬
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.repo_path = Path(repo_path)
        self.artifacts_dir = Path(artifacts_dir)
        self.data_dir = Path(data_dir)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.artifacts_dir.mkdir(exist_ok=True)
        self.data_dir.mkdir(exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.parser = CodeParser()
        self.chunker = CodeChunker()
        
        # FAISS ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš© (ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰)
        self.vector_store = FAISSStore(
            index_path=str(self.artifacts_dir / "faiss.index"),
            dimension=3072 
        )
        logger.info("FAISS ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # LLM Analyzer ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ ë¬¸ì œ ì‹œ Noneìœ¼ë¡œ ì„¤ì •)
        try:
            self.llm_analyzer = LLMAnalyzer()
            logger.info("LLM Analyzer ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"LLM Analyzer ì´ˆê¸°í™” ì‹¤íŒ¨ (LLM ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
            self.llm_analyzer = None
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.sqlite_store = SQLiteStore(self.data_dir / "structsynth_code.db")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {}
        self.run_id = None
        
        logger.info(f"StructSynthAgent ì´ˆê¸°í™” ì™„ë£Œ: {self.repo_path}")
    
    def analyze_repository(self) -> Dict[str, Any]:
        """
        ì €ì¥ì†Œ ì „ì²´ ë¶„ì„ ìˆ˜í–‰
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ì €ì¥ì†Œ ë¶„ì„ ì‹œì‘: {self.repo_path}")
        
        try:
            # ì‹¤í–‰ ì„¸ì…˜ ì‹œì‘
            self.run_id = self.sqlite_store.insert_run(
                agent_name="StructSynth",
                input_summary=f"Repository: {self.repo_path}",
                metadata={
                    "repo_path": str(self.repo_path),
                    "start_time": datetime.now().isoformat(),
                    "status": "running"
                }
            )
            
            # 1. ì½”ë“œ íŒŒì‹± ë° AST ì¶”ì¶œ
            logger.info("1ë‹¨ê³„: ì½”ë“œ íŒŒì‹± ë° AST ì¶”ì¶œ")
            parsed_data = self._parse_repository()
            
            # 2. íŒŒì¼ ë° ì‹¬ë³¼ ë ˆë²¨ LLM ë¶„ì„
            logger.info("2ë‹¨ê³„: íŒŒì¼ ë° ì‹¬ë³¼ ë ˆë²¨ LLM ë¶„ì„")
            parsed_data = self._perform_file_symbol_llm_analysis(parsed_data)
            
            # 3. ì½”ë“œ ì²­í‚¹
            logger.info("3ë‹¨ê³„: ì½”ë“œ ì²­í‚¹")
            chunked_data = self._chunk_code(parsed_data)
            
            # 4. ì²­í‚¹ ë‹¨ìœ„ LLM ë¶„ì„
            logger.info("4ë‹¨ê³„: ì²­í‚¹ ë‹¨ìœ„ LLM ë¶„ì„")
            chunked_data = self._perform_chunk_llm_analysis(chunked_data)
            
            # 5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            logger.info("5ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
            self._save_to_database(parsed_data, chunked_data)
            
            # 5-1. JSON íŒŒì¼ ìë™ ì €ì¥
            logger.info("5-1ë‹¨ê³„: JSON íŒŒì¼ ìë™ ì €ì¥")
            self._save_metadata_to_json(parsed_data, chunked_data)
            
            # 6. ë²¡í„° ì„ë² ë”© ìƒì„±
            logger.info("6ë‹¨ê³„: ë²¡í„° ì„ë² ë”© ìƒì„±")
            try:
                self._create_embeddings(chunked_data)
                logger.info("ë²¡í„° ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ë²¡í„° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ë¶„ì„ì€ ì™„ë£Œë¨): {e}")
                # ë²¡í„° ìƒì„± ì‹¤íŒ¨í•´ë„ ë¶„ì„ ê²°ê³¼ëŠ” ì´ë¯¸ ì €ì¥ë¨
            
            # ì‹¤í–‰ ì™„ë£Œ (ë²¡í„° ìƒì„± ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
            self.sqlite_store.update_run_status(
                self.run_id, 
                "completed",
                finished_at=datetime.now().isoformat(),
                output_summary=f"Analysis completed: {parsed_data['total_files']} files, {parsed_data['total_symbols']} symbols (ë²¡í„° ìƒì„±: {'ì„±ê³µ' if 'ë²¡í„° ì„ë² ë”© ìƒì„± ì™„ë£Œ' in locals() else 'ì‹¤íŒ¨'})"
            )
            
            # ê²°ê³¼ ì •ë¦¬
            self.analysis_results = {
                "parsed_data": parsed_data,
                "chunked_data": chunked_data,
                "run_id": self.run_id,
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info("ì €ì¥ì†Œ ë¶„ì„ ì™„ë£Œ")
            return self.analysis_results
            
        except Exception as e:
            logger.error(f"ì €ì¥ì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            if self.run_id:
                self.sqlite_store.update_run_status(
                    self.run_id, 
                    "failed", 
                    finished_at=datetime.now().isoformat(),
                    output_summary=f"Analysis failed: {str(e)}"
                )
            raise
    
    def _parse_repository(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ ì „ì²´ íŒŒì‹±"""
        parsed_data = {
            "files": [],
            "symbols": [],
            "total_files": 0,
            "total_symbols": 0
        }
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        supported_extensions = {'.py', '.js', '.ts', '.java', '.cpp', '.c', '.go'}
        
        for file_path in self.repo_path.rglob('*'):
            if file_path.is_file() and file_path.suffix in supported_extensions:
                try:
                    file_data = self.parser.parse_file(file_path)
                    if file_data:
                        parsed_data["files"].append(file_data)
                        parsed_data["symbols"].extend(file_data.get("symbols", []))
                        parsed_data["total_files"] += 1
                        parsed_data["total_symbols"] += len(file_data.get("symbols", []))
                        
                except Exception as e:
                    logger.warning(f"íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨ {file_path}: {e}")
        
        logger.info(f"íŒŒì‹± ì™„ë£Œ: {parsed_data['total_files']}ê°œ íŒŒì¼, {parsed_data['total_symbols']}ê°œ ì‹¬ë³¼")
        return parsed_data
    
    def _chunk_code(self, parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì½”ë“œ ì²­í‚¹ ìˆ˜í–‰"""
        chunked_data = {
            "chunks": [],
            "total_chunks": 0
        }
        
        for file_data in parsed_data["files"]:
            try:
                file_chunks = self.chunker.chunk_file(file_data)
                chunked_data["chunks"].extend(file_chunks)
                chunked_data["total_chunks"] += len(file_chunks)
                
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì²­í‚¹ ì‹¤íŒ¨ {file_data.get('path')}: {e}")
        
        logger.info(f"ì²­í‚¹ ì™„ë£Œ: {chunked_data['total_chunks']}ê°œ ì²­í¬")
        return chunked_data
    
    def _perform_file_symbol_llm_analysis(self, parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """íŒŒì¼ ë° ì‹¬ë³¼ ë ˆë²¨ LLM ë¶„ì„ ìˆ˜í–‰"""
        if not self.llm_analyzer:
            logger.warning("LLM Analyzerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ íŒŒì¼/ì‹¬ë³¼ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return parsed_data
        
        try:
            analyzed_files = []
            
            for file_data in parsed_data["files"]:
                try:
                    file_path = file_data.get("file_path", "")
                    logger.info(f"íŒŒì¼ LLM ë¶„ì„ ì‹œì‘: {file_path}")
                    
                    # íŒŒì¼ ë‚´ìš© ì½ê¸° (ì»¨í…ìŠ¤íŠ¸ìš©)
                    file_context = ""
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            file_context = f.read()
                    except Exception as e:
                        logger.warning(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
                    
                    # íŒŒì¼ ë ˆë²¨ LLM ë¶„ì„
                    file_llm_analysis = self.llm_analyzer.analyze_file(file_data, file_context)
                    
                    # íŒŒì¼ ë°ì´í„°ì— LLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€
                    enriched_file = file_data.copy()
                    enriched_file["llm_summary"] = file_llm_analysis.get("summary", "")
                    enriched_file["llm_analysis"] = file_llm_analysis
                    
                    # ì‹¬ë³¼ë³„ LLM ë¶„ì„
                    enriched_symbols = []
                    for symbol in file_data.get("symbols", []):
                        try:
                            symbol_type = symbol.get("type", "")
                            
                            if symbol_type == "function":
                                symbol_llm_analysis = self.llm_analyzer.analyze_function(symbol, file_context)
                            elif symbol_type == "class":
                                symbol_llm_analysis = self.llm_analyzer.analyze_class(symbol, file_context)
                            else:
                                # ë³€ìˆ˜ë‚˜ ë‹¤ë¥¸ íƒ€ì…ì˜ ì‹¬ë³¼
                                symbol_llm_analysis = {
                                    "llm_summary": f"{symbol_type} ì‹¬ë³¼",
                                    "responsibility": "ë³€ìˆ˜ ë˜ëŠ” ê¸°íƒ€ ì‹¬ë³¼",
                                    "design_notes": "ê¸°ë³¸ ì‹¬ë³¼",
                                    "collaboration": "ê¸°ë³¸ ì‹¬ë³¼",
                                    "llm_analysis": {}
                                }
                            
                            # ì‹¬ë³¼ì— LLM ë¶„ì„ ê²°ê³¼ ì¶”ê°€ - symbols í…Œì´ë¸” ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜
                            enriched_symbol = symbol.copy()
                            enriched_symbol["llm_summary"] = symbol_llm_analysis.get("llm_summary", "")
                            enriched_symbol["responsibility"] = symbol_llm_analysis.get("responsibility", "")
                            enriched_symbol["design_notes"] = symbol_llm_analysis.get("design_notes", "")
                            enriched_symbol["collaboration"] = symbol_llm_analysis.get("collaboration", "")
                            enriched_symbol["llm_analysis"] = symbol_llm_analysis.get("llm_analysis", {})
                            
                            enriched_symbols.append(enriched_symbol)
                            
                        except Exception as e:
                            logger.warning(f"ì‹¬ë³¼ LLM ë¶„ì„ ì‹¤íŒ¨ {symbol.get('name', 'unknown')}: {e}")
                            # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‹¬ë³¼ ìœ ì§€
                            symbol["llm_summary"] = "ë¶„ì„ ì‹¤íŒ¨"
                            symbol["responsibility"] = "ë¶„ì„ ì‹¤íŒ¨"
                            symbol["design_notes"] = "ë¶„ì„ ì‹¤íŒ¨"
                            symbol["collaboration"] = "ë¶„ì„ ì‹¤íŒ¨"
                            symbol["llm_analysis"] = {"error": str(e)}
                            enriched_symbols.append(symbol)
                    
                    # íŒŒì¼ì˜ ì‹¬ë³¼ì„ ë¶„ì„ëœ ì‹¬ë³¼ë¡œ êµì²´
                    enriched_file["symbols"] = enriched_symbols
                    analyzed_files.append(enriched_file)
                    
                    logger.info(f"íŒŒì¼ LLM ë¶„ì„ ì™„ë£Œ: {file_path}")
                    
                except Exception as e:
                    logger.warning(f"íŒŒì¼ LLM ë¶„ì„ ì‹¤íŒ¨ {file_data.get('path', 'unknown')}: {e}")
                    # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ë°ì´í„° ìœ ì§€
                    analyzed_files.append(file_data)
            
            # ë¶„ì„ëœ íŒŒì¼ë¡œ ì—…ë°ì´íŠ¸
            parsed_data["files"] = analyzed_files
            
            # ì „ì²´ ì‹¬ë³¼ ëª©ë¡ë„ ì—…ë°ì´íŠ¸
            all_symbols = []
            for file_data in analyzed_files:
                all_symbols.extend(file_data.get("symbols", []))
            parsed_data["symbols"] = all_symbols
            
            logger.info(f"íŒŒì¼ ë° ì‹¬ë³¼ LLM ë¶„ì„ ì™„ë£Œ: {len(analyzed_files)}ê°œ íŒŒì¼, {len(all_symbols)}ê°œ ì‹¬ë³¼")
            return parsed_data
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë° ì‹¬ë³¼ LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            return parsed_data
    
    def _create_embeddings(self, chunked_data: Dict[str, Any]):
        """ë²¡í„° ì„ë² ë”© ìƒì„±"""
        try:
            vectors = []
            doc_ids = []
            
            for i, chunk in enumerate(chunked_data["chunks"]):
                try:
                    # ì²­í¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    chunk_text = self._chunk_to_text(chunk)
                    
                    # ì„ë² ë”© ìƒì„± (LLM Analyzer ì‚¬ìš©)
                    if self.llm_analyzer:
                        embedding = self.llm_analyzer.create_embedding(chunk_text)
                        if embedding:
                            vectors.append(embedding)
                            # ì‹¤ì œ chunk ID ì‚¬ìš© (enumerate ì¸ë±ìŠ¤ê°€ ì•„ë‹Œ)
                            chunk_id = chunk.get("id", i + 1)  # i+1ë¡œ 1ë¶€í„° ì‹œì‘
                            doc_ids.append(chunk_id)
                            logger.debug(f"ì²­í¬ {chunk_id} ì„ë² ë”© ìƒì„± ì™„ë£Œ")
                        else:
                            logger.warning(f"ì²­í¬ {i} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                    else:
                        logger.warning("LLM Analyzerê°€ ì—†ì–´ ì„ë² ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤")
                        break
                        
                except Exception as e:
                    logger.warning(f"ì²­í¬ {i} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            # FAISS ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
            if vectors:
                self.vector_store.add_vectors(vectors, doc_ids)
                self.vector_store.save_index()
                
                # SQLite embeddings í…Œì´ë¸”ì— ì €ì¥
                self._save_embeddings_to_sqlite(vectors, doc_ids)
                
                logger.info(f"ë²¡í„° ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(vectors)}ê°œ")
            else:
                logger.warning("ìƒì„±ëœ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
            
        except Exception as e:
            logger.error(f"ë²¡í„° ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def _chunk_to_text(self, chunk: Dict[str, Any]) -> str:
        """ì²­í¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        # ì²­í¬ íƒ€ì…ë³„ í…ìŠ¤íŠ¸ êµ¬ì„±
        if chunk.get("content"):
            text_parts.append(chunk["content"])
        
        if chunk.get("llm_analysis"):
            analysis = chunk["llm_analysis"]
            if isinstance(analysis, dict):
                for key, value in analysis.items():
                    if value and str(value) != "ë¶„ì„ ì‹¤íŒ¨":
                        text_parts.append(f"{key}: {value}")
            else:
                text_parts.append(str(analysis))
        
        if chunk.get("symbol_name"):
            text_parts.append(f"Symbol: {chunk['symbol_name']}")
        
        if chunk.get("chunk_type"):
            text_parts.append(f"Type: {chunk['chunk_type']}")
        
        return " | ".join(text_parts)
    
    def _perform_chunk_llm_analysis(self, chunked_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì²­í‚¹ ë‹¨ìœ„ LLM ë¶„ì„ ìˆ˜í–‰"""
        try:
            analyzed_chunks = []
            
            for i, chunk in enumerate(chunked_data["chunks"]):
                try:
                    # ì²­í¬ë³„ LLM ë¶„ì„
                    chunk_analysis = self.llm_analyzer.analyze_chunk(chunk)
                    
                    # ë¶„ì„ ê²°ê³¼ë¥¼ ì²­í¬ì— ì¶”ê°€
                    enriched_chunk = chunk.copy()
                    enriched_chunk["llm_analysis"] = chunk_analysis
                    enriched_chunk["chunk_id"] = i
                    
                    analyzed_chunks.append(enriched_chunk)
                    
                    logger.info(f"ì²­í¬ {i} LLM ë¶„ì„ ì™„ë£Œ: {chunk.get('symbol_name', 'unknown')}")
                    
                except Exception as e:
                    logger.warning(f"ì²­í¬ {i} LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
                    # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì²­í¬ ìœ ì§€
                    chunk["llm_analysis"] = {"error": str(e)}
                    analyzed_chunks.append(chunk)
            
            # ë¶„ì„ëœ ì²­í¬ë¡œ ì—…ë°ì´íŠ¸
            chunked_data["chunks"] = analyzed_chunks
            chunked_data["analyzed_chunks"] = len(analyzed_chunks)
            
            logger.info(f"ì²­í‚¹ ë‹¨ìœ„ LLM ë¶„ì„ ì™„ë£Œ: {len(analyzed_chunks)}ê°œ ì²­í¬")
            return chunked_data
            
        except Exception as e:
            logger.error(f"ì²­í‚¹ ë‹¨ìœ„ LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            return chunked_data
    
    def _save_to_database(self, parsed_data: Dict[str, Any], chunked_data: Dict[str, Any]):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            # ê° íŒŒì¼ë³„ë¡œ AST ë°ì´í„° ì €ì¥
            for file_data in parsed_data["files"]:
                try:
                    # íŒŒì¼ ë°ì´í„°ë¥¼ save_ast_data í˜•ì‹ì— ë§ê²Œ ë³€í™˜
                    ast_data = {
                        "file": {
                            "path": file_data.get("file_path", ""),
                            "language": file_data.get("language", "unknown"),
                            "llm_summary": file_data.get("llm_summary")
                        },
                        "symbols": file_data.get("symbols", [])
                    }
                    
                    # AST ë°ì´í„° ì €ì¥
                    self.sqlite_store.save_ast_data(ast_data)
                    
                except Exception as e:
                    logger.warning(f"íŒŒì¼ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ {file_data.get('path', 'unknown')}: {e}")
                    continue
            
            # ì²­í¬ ë°ì´í„°ë¥¼ chunks í…Œì´ë¸”ì— ì €ì¥
            chunks_saved = 0
            for chunk in chunked_data.get("chunks", []):
                try:
                    # symbol_idëŠ” chunkì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì„ì‹œë¡œ 1 ì‚¬ìš©
                    symbol_id = chunk.get("symbol_id", 1)
                    chunk_type = chunk.get("chunk_type", "code")
                    content = chunk.get("content", "")
                    tokens = chunk.get("tokens", len(content.split()) if content else 0)
                    
                    # ì²­í¬ ì €ì¥
                    chunk_id = self.sqlite_store.insert_chunk(
                        symbol_id=symbol_id,
                        chunk_type=chunk_type,
                        content=content,
                        tokens=tokens
                    )
                    
                    if chunk_id:
                        chunks_saved += 1
                        logger.debug(f"ì²­í¬ ì €ì¥ ì™„ë£Œ: ID {chunk_id}, íƒ€ì…: {chunk_type}")
                    
                except Exception as e:
                    logger.warning(f"ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {chunks_saved}ê°œ ì²­í¬ ì €ì¥ë¨")
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _perform_llm_analysis(self, parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ ë¶„ì„"""
        try:
            # LLMAnalyzerì˜ í†µí•© ë¶„ì„ ë©”ì„œë“œ í˜¸ì¶œ
            analysis_results = self.llm_analyzer.perform_repository_analysis(parsed_data)
            
            logger.info("LLM ë¶„ì„ ì™„ë£Œ")
            return analysis_results
            
        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def get_vector_store_stats(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ í†µê³„ ë°˜í™˜"""
        try:
            return self.vector_store.get_stats()
        except Exception as e:
            logger.error(f"ë²¡í„° ìŠ¤í† ì–´ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def search_symbols(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """ì‹¬ë³¼ ê²€ìƒ‰"""
        try:
            # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.vector_store.search(query, top_k)
            
            # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš° (API í‚¤ ì—†ìŒ, ê²°ê³¼ ì—†ìŒ, ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìŒ)
            if not search_results or all(r.get("similarity", 0) < 0.1 for r in search_results):
                logger.info(f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. SQLiteì—ì„œ ì§ì ‘ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                # ë²¡í„° ê²€ìƒ‰ì´ ì‹¤íŒ¨í•˜ë©´ SQLiteì—ì„œ ì§ì ‘ ê²€ìƒ‰
                search_results = self._search_symbols_in_sqlite(query, top_k)
            
            if not search_results:
                logger.info(f"ì¿¼ë¦¬ '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ê²€ìƒ‰ ê²°ê³¼ì— ì‹¤ì œ ì‹¬ë³¼ ì •ë³´ ì¶”ê°€
            enriched_results = []
            for result in search_results:
                doc_id = result.get("doc_id")
                if doc_id:
                    # SQLiteì—ì„œ ì‹¬ë³¼ ì •ë³´ ì¡°íšŒ
                    symbol_info = self.sqlite_store.get_symbol_by_id(doc_id)
                    if symbol_info:
                        enriched_result = {
                            **result,
                            "symbol_info": symbol_info
                        }
                        enriched_results.append(enriched_result)
                    else:
                        # ì‹¬ë³¼ ì •ë³´ê°€ ì—†ì–´ë„ ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ëŠ” í¬í•¨
                        enriched_results.append(result)
                else:
                    enriched_results.append(result)
            
            logger.info(f"ì¿¼ë¦¬ '{query}'ì— ëŒ€í•œ {len(enriched_results)}ê°œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜")
            return enriched_results
            
        except Exception as e:
            logger.error(f"ì‹¬ë³¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ SQLite ê²€ìƒ‰ ì‹œë„
            try:
                return self._search_symbols_in_sqlite(query, top_k)
            except Exception as fallback_error:
                logger.error(f"SQLite fallback ê²€ìƒ‰ë„ ì‹¤íŒ¨: {fallback_error}")
                return []
    
    def _search_symbols_in_sqlite(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """SQLiteì—ì„œ ì§ì ‘ ì‹¬ë³¼ ê²€ìƒ‰ (fallback)"""
        try:
            # ì¿¼ë¦¬ ì •ê·œí™”
            query_lower = query.lower().strip()
            logger.info(f"SQLiteì—ì„œ ì§ì ‘ ê²€ìƒ‰ ì‹œë„: '{query}' -> '{query_lower}'")
            
            matching_chunks = []
            
            with sqlite3.connect(self.sqlite_store.db_path) as conn:
                # 1. symbols í…Œì´ë¸”ì—ì„œ ì§ì ‘ í•¨ìˆ˜ëª…/í´ë˜ìŠ¤ëª… ê²€ìƒ‰
                cursor = conn.execute("""
                    SELECT id, name, type, file_path, start_line, end_line, content
                    FROM symbols 
                    WHERE LOWER(name) LIKE ? OR LOWER(content) LIKE ?
                    ORDER BY id
                    LIMIT ?
                """, (f'%{query_lower}%', f'%{query_lower}%', top_k))
                
                symbols = cursor.fetchall()
                logger.info(f"symbols í…Œì´ë¸”ì—ì„œ {len(symbols)}ê°œ ê²°ê³¼ ë°œê²¬")
                
                for symbol in symbols:
                    symbol_id, name, symbol_type, file_path, start_line, end_line, content = symbol
                    
                    # ì •í™•í•œ ë§¤ì¹­ì¸ ê²½ìš° ë†’ì€ ìœ ì‚¬ë„ ë¶€ì—¬
                    similarity = 1.0 if query_lower in name.lower() else 0.8
                    
                    matching_chunks.append({
                        "doc_id": symbol_id,
                        "similarity": similarity,
                        "index": len(matching_chunks),
                        "symbol_info": {
                            "name": name,
                            "type": symbol_type,
                            "file_path": file_path,
                            "start_line": start_line,
                            "end_line": end_line,
                            "content": content
                        },
                        "chunk_content": content[:200] + "..." if len(content) > 200 else content
                    })
                
                # 2. chunks í…Œì´ë¸”ì—ì„œë„ ê²€ìƒ‰ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸)
                if len(matching_chunks) < top_k:
                    cursor = conn.execute("""
                        SELECT c.id, c.symbol_id, c.chunk_type, c.content, c.symbol_id
                        FROM chunks c
                        WHERE LOWER(c.content) LIKE ? OR LOWER(c.symbol_id) LIKE ?
                        ORDER BY c.id
                        LIMIT ?
                    """, (f'%{query_lower}%', f'%{query_lower}%', top_k - len(matching_chunks)))
                    
                    chunks = cursor.fetchall()
                    logger.info(f"chunks í…Œì´ë¸”ì—ì„œ {len(chunks)}ê°œ ì¶”ê°€ ê²°ê³¼ ë°œê²¬")
                    
                    for chunk in chunks:
                        chunk_id, symbol_id, chunk_type, content, symbol_id_str = chunk
                        
                        # symbol_idì—ì„œ ì‹¤ì œ ì‹¬ë³¼ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: "1_SimpleClass" -> "SimpleClass")
                        if '_' in symbol_id_str:
                            actual_symbol_name = symbol_id_str.split('_', 1)[1]
                        else:
                            actual_symbol_name = symbol_id_str
                        
                        # ì‹¬ë³¼ ì •ë³´ ì¡°íšŒ ì‹œë„
                        symbol_info = None
                        try:
                            # symbol_idê°€ ìˆ«ìì¸ ê²½ìš° ì§ì ‘ ì¡°íšŒ
                            if symbol_id_str.isdigit():
                                symbol_info = self.sqlite_store.get_symbol_by_id(int(symbol_id_str))
                            else:
                                # symbol_idê°€ ë¬¸ìì—´ì¸ ê²½ìš° ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
                                all_symbols = self.sqlite_store.get_all_symbols()
                                for symbol in all_symbols:
                                    if symbol.get('name') == actual_symbol_name:
                                        symbol_info = symbol
                                        break
                        except:
                            pass
                        
                        # ì¤‘ë³µ ì œê±° (ì´ë¯¸ symbolsì—ì„œ ì°¾ì€ ê²½ìš°)
                        if not any(chunk["symbol_info"]["name"] == actual_symbol_name for chunk in matching_chunks):
                            matching_chunks.append({
                                "doc_id": chunk_id,
                                "similarity": 0.9,  # ë†’ì€ ìœ ì‚¬ë„ (ì§ì ‘ ë§¤ì¹­)
                                "index": len(matching_chunks),
                                "symbol_info": symbol_info or {"name": actual_symbol_name, "type": chunk_type},
                                "chunk_content": content[:200] + "..." if len(content) > 200 else content
                            })
                
                # 3. ê²°ê³¼ ì •ë ¬ (ìœ ì‚¬ë„ ê¸°ì¤€)
                matching_chunks.sort(key=lambda x: x["similarity"], reverse=True)
                matching_chunks = matching_chunks[:top_k]
                
                logger.info(f"SQLite ê²€ìƒ‰ ì™„ë£Œ: ì´ {len(matching_chunks)}ê°œ ê²°ê³¼")
                return matching_chunks
                
        except Exception as e:
            logger.error(f"SQLite ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_analysis_summary(self) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        if not self.analysis_results:
            return {"status": "no_analysis_performed"}
        
        return {
            "status": "completed",
            "run_id": self.run_id,
            "files_analyzed": self.analysis_results.get("parsed_data", {}).get("total_files", 0),
            "symbols_found": self.analysis_results.get("parsed_data", {}).get("total_symbols", 0),
            "chunks_created": self.analysis_results.get("chunked_data", {}).get("total_chunks", 0),
            "timestamp": self.analysis_results.get("timestamp"),
            "vector_store_stats": self.get_vector_store_stats()
        }
    
    def _save_embeddings_to_sqlite(self, vectors: List[List[float]], doc_ids: List[int]):
        """ì„ë² ë”©ì„ SQLite embeddings í…Œì´ë¸”ì— ì €ì¥"""
        try:
            embeddings_saved = 0
            
            for i, (vector, doc_id) in enumerate(zip(vectors, doc_ids)):
                try:
                    # ë²¡í„°ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜
                    import numpy as np
                    vector_bytes = np.array(vector, dtype=np.float32).tobytes()
                    
                    # embeddings í…Œì´ë¸”ì— ì €ì¥
                    embedding_id = self.sqlite_store.insert_embedding(
                        object_type="chunk",
                        object_id=doc_id,
                        vector=vector_bytes,
                        dimension=len(vector)
                    )
                    
                    if embedding_id:
                        embeddings_saved += 1
                        logger.debug(f"ì„ë² ë”© {i} ì €ì¥ ì™„ë£Œ: ID {embedding_id}, ì°¨ì›: {len(vector)}")
                        
                        # chunks í…Œì´ë¸”ì˜ embedding_id ì—…ë°ì´íŠ¸
                        try:
                            self.sqlite_store.update_chunk_embedding(doc_id, embedding_id)
                            logger.debug(f"ì²­í¬ {doc_id}ì˜ embedding_id ì—…ë°ì´íŠ¸ ì™„ë£Œ: {embedding_id}")
                        except Exception as e:
                            logger.warning(f"ì²­í¬ {doc_id}ì˜ embedding_id ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                    
                except Exception as e:
                    logger.warning(f"ì„ë² ë”© {i} ì €ì¥ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"SQLite embeddings ì €ì¥ ì™„ë£Œ: {embeddings_saved}ê°œ ì„ë² ë”©")
            
        except Exception as e:
            logger.error(f"SQLite embeddings ì €ì¥ ì‹¤íŒ¨: {e}")
            # ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨í•´ë„ ì „ì²´ ë¶„ì„ì€ ê³„ì† ì§„í–‰
    
    def _save_metadata_to_json(self, parsed_data: Dict[str, Any], chunked_data: Dict[str, Any]):
        """ë¶„ì„ ê²°ê³¼ë¥¼ metadata.json íŒŒì¼ë¡œ ìë™ ì €ì¥"""
        try:
            # ì‚¬ìš©ì ìš”ì²­ êµ¬ì¡°ì— ë§ì¶˜ metadata.json ìƒì„±
            metadata = []
            
            for symbol in parsed_data.get("symbols", []):
                # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                file_path = "unknown"
                for file_data in parsed_data.get("files", []):
                    if symbol in file_data.get("symbols", []):
                        file_path = file_data.get("file_path", "unknown")
                        break
                
                # ìœ„ì¹˜ ì •ë³´
                location = {}
                if symbol.get("location"):
                    loc = symbol["location"]
                    if loc.get("start_line") and loc.get("end_line"):
                        location = {
                            "start_line": loc["start_line"],
                            "end_line": loc["end_line"]
                        }
                
                # LLM ë¶„ì„ ì •ë³´
                llm_analysis = {}
                if symbol.get("llm_summary"):
                    llm_analysis["summary"] = symbol["llm_summary"]
                if symbol.get("responsibility"):
                    llm_analysis["responsibility"] = symbol["responsibility"]
                if symbol.get("design_notes"):
                    llm_analysis["design_notes"] = symbol["design_notes"]
                if symbol.get("collaboration"):
                    llm_analysis["collaboration"] = symbol["collaboration"]
                
                # í…ìŠ¤íŠ¸ ë‚´ìš©
                text_content = f"Symbol: {symbol.get('name', 'unknown')} | Type: {symbol.get('type', 'unknown')} | File: {file_path}"
                
                # ì‹¬ë³¼ ë°ì´í„° êµ¬ì„±
                symbol_data = {
                    "file_path": file_path,
                    "symbol_type": symbol.get("type", "unknown"),
                    "symbol_name": symbol.get("name", "unknown"),
                    "location": location,
                    "llm_analysis": llm_analysis,
                    "text_content": text_content,
                    "timestamp": datetime.now().isoformat()
                }
                
                metadata.append(symbol_data)
            
            # metadata.json íŒŒì¼ë¡œ ì €ì¥
            output_path = self.artifacts_dir / "metadata.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… metadata.json ìë™ ì €ì¥ ì™„ë£Œ: {output_path}")
            logger.info(f"ğŸ“Š ì´ {len(metadata)}ê°œ ì‹¬ë³¼ ì •ë³´ ì €ì¥ë¨")
            
        except Exception as e:
            logger.error(f"âŒ metadata.json ìë™ ì €ì¥ ì‹¤íŒ¨: {e}")
            # JSON ì €ì¥ ì‹¤íŒ¨í•´ë„ ì „ì²´ ë¶„ì„ì€ ê³„ì† ì§„í–‰