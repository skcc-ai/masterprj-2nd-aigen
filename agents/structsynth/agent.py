"""
StructSynth Agent - ì½”ë“œ êµ¬ì¡° ë¶„ì„ ë° AST ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
Tree-sitterë¥¼ í™œìš©í•´ ì½”ë“œ êµ¬ì¡°ë¥¼ ì¶”ì¶œ/ì •ì œí•˜ì—¬ AST JSON, SQLite, FAISSì— ì €ì¥
LLMì„ í†µí•œ í•¨ìˆ˜/ë©”ì„œë“œ ë¶„ì„ ë° ë²¡í„° DB ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€
"""

import logging
import os
import json
from pathlib import Path
from typing import Dict, List, Any, Optional
from .parser import CodeParser
from .chunker import CodeChunker
from .indexer import CodeIndexer
from .llm_analyzer import LLMAnalyzer
from .vector_store import VectorStore
from datetime import datetime

logger = logging.getLogger(__name__)


class StructSynthAgent:
    def __init__(self, repo_path: str, artifacts_dir: str = "../artifacts", data_dir: str = "../test_file"):
        self.repo_path = Path(repo_path)
        self.artifacts_dir = Path(artifacts_dir)
        self.data_dir = Path(data_dir)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.artifacts_dir.mkdir(exist_ok=True)
        self.data_dir.mkdir(exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.parser = CodeParser()
        self.chunker = CodeChunker()
        
        # Azure OpenAI API keyê°€ ìˆìœ¼ë©´ LLM ë¶„ì„ê¸°ì™€ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        try:
            self.llm_analyzer = LLMAnalyzer()
            self.vector_store = VectorStore(storage_dir=str(self.artifacts_dir / "vector_store"))
            logger.info("âœ… LLM ë¶„ì„ê¸°ì™€ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸  LLM ë¶„ì„ê¸°/ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨ (Azure OpenAI API key ì—†ìŒ): {e}")
            self.llm_analyzer = None
            self.vector_store = None
        
        # Azure OpenAI API keyê°€ ìˆìœ¼ë©´ indexer ì´ˆê¸°í™”, ì—†ìœ¼ë©´ None
        try:
            self.indexer = CodeIndexer()
            logger.info("âœ… CodeIndexer ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.warning(f"âš ï¸  CodeIndexer ì´ˆê¸°í™” ì‹¤íŒ¨ (Azure OpenAI API key ì—†ìŒ): {e}")
            self.indexer = None
        
        # ì§€ì› ì–¸ì–´ ë° ì œì™¸ íŒ¨í„´
        self.supported_extensions = {'.py', '.java', '.c'}
        self.exclude_patterns = [
            '.git', 'node_modules', '__pycache__', '.pytest_cache',
            'build', 'dist', 'target', 'bin', 'obj', 'venv', 'env',
            '.venv', '.env', 'site-packages'
        ]

    def run(self, repo_path: str):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
        self.analyze_repository()
    
    def analyze_repository(self) -> None:
        """ì €ì¥ì†Œ ë¶„ì„ ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
        logger.info(f"ğŸš€ ì €ì¥ì†Œ ë¶„ì„ ì‹œì‘: {self.repo_path}")
        
        # 1. ì €ì¥ì†Œ ìŠ¤ìº” ë° íŒŒì¼ ìˆ˜ì§‘
        files = self._scan_repository()
        logger.info(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(files)}")
        
        # 2. ê° íŒŒì¼ ë¶„ì„ ë° AST ì¶”ì¶œ
        all_ast_data = []
        for file_path in files:
            try:
                ast_data = self._analyze_file(file_path)
                if ast_data:
                    all_ast_data.append(ast_data)
            except Exception as e:
                logger.error(f"âš ï¸  íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
        
        # 3. LLMì„ í†µí•œ ì‹¬ë³¼ ë¶„ì„ (í•¨ìˆ˜/ë©”ì„œë“œ/í´ë˜ìŠ¤)
        if self.llm_analyzer:
            all_ast_data = self._analyze_symbols_with_llm(all_ast_data)
        
        # 4. ë²¡í„° DB ì €ì¥
        if self.vector_store:
            self._save_to_vector_store(all_ast_data)
        
        # 5. AST JSON ì €ì¥
        self._save_ast_json(all_ast_data)
        
        # 6. SQLite ì €ì¥ (êµ¬ì¡°ë§Œ)
        self._save_to_sqlite(all_ast_data)
        
        # 7. FAISS ì €ì¥ (êµ¬ì¡°ë§Œ)
        self._save_to_faiss(all_ast_data)
        
        logger.info(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ! {len(all_ast_data)}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨")
    
    def _scan_repository(self) -> List[Path]:
        """ì €ì¥ì†Œ ìŠ¤ìº”í•˜ì—¬ ë¶„ì„í•  íŒŒì¼ë“¤ ìˆ˜ì§‘"""
        files = []
        
        logger.info(f"ğŸ” ì €ì¥ì†Œ ìŠ¤ìº” ì‹œì‘: {self.repo_path}")
        logger.info(f"ğŸ“‹ ì§€ì› í™•ì¥ì: {self.supported_extensions}")
        logger.info(f"ğŸš« ì œì™¸ íŒ¨í„´: {self.exclude_patterns}")
        
        # repo_pathê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not self.repo_path.exists():
            logger.error(f"âŒ ì €ì¥ì†Œ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {self.repo_path}")
            return files
        
        if not self.repo_path.is_dir():
            logger.error(f"âŒ ì €ì¥ì†Œ ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹˜: {self.repo_path}")
            return files
        
        logger.info(f"ğŸ“ ì €ì¥ì†Œ ê²½ë¡œ í™•ì¸ë¨: {self.repo_path}")
        
        for root, dirs, filenames in os.walk(self.repo_path):
            logger.debug(f"ğŸ” ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘: {root}")
            logger.debug(f"ğŸ“ ë°œê²¬ëœ í•˜ìœ„ ë””ë ‰í† ë¦¬: {dirs}")
            logger.debug(f"ğŸ“„ ë°œê²¬ëœ íŒŒì¼ë“¤: {filenames}")
            
            # ì œì™¸í•  ë””ë ‰í† ë¦¬ í•„í„°ë§
            dirs[:] = [d for d in dirs if d not in self.exclude_patterns]
            logger.debug(f"âœ… í•„í„°ë§ í›„ í•˜ìœ„ ë””ë ‰í† ë¦¬: {dirs}")
            
            for filename in filenames:
                file_path = Path(root) / filename
                ext = file_path.suffix.lower()
                
                logger.debug(f"ğŸ” íŒŒì¼ ê²€ì‚¬: {file_path} (í™•ì¥ì: {ext})")
                
                if ext in self.supported_extensions:
                    files.append(file_path)
                    logger.info(f"âœ… ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ë°œê²¬: {file_path}")
                else:
                    logger.debug(f"â­ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì ê±´ë„ˆëœ€: {file_path} ({ext})")
        
        logger.info(f"ğŸ“Š ìŠ¤ìº” ì™„ë£Œ: ì´ {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
        return files

    def _analyze_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """ê°œë³„ íŒŒì¼ ë¶„ì„ ë° AST ì¶”ì¶œ"""
        try:
            logger.info(f"ğŸ” íŒŒì¼ ë¶„ì„ ì‹œì‘: {file_path}")
            
            # íŒŒì¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()
            
            # í™•ì¥ì í™•ì¸
            ext = file_path.suffix.lower()
            if ext not in self.parser.parsers:
                logger.warning(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” í™•ì¥ì: {ext}")
                return None
            
            # AST ì¶”ì¶œ
            extractor = self.parser.get_extractor(ext)
            ast_data = extractor.parse_to_ast(code, str(file_path))
            
            logger.info(f"âœ… íŒŒì¼ ë¶„ì„ ì™„ë£Œ: {file_path} -> {len(ast_data.get('symbols', []))}ê°œ ì‹¬ë³¼")
            return ast_data
            
        except Exception as e:
            logger.error(f"âš ï¸  íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            return None

    def _analyze_symbols_with_llm(self, all_ast_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """LLMì„ í†µí•œ ì‹¬ë³¼ ë¶„ì„ ìˆ˜í–‰"""
        if not self.llm_analyzer:
            return all_ast_data
        
        logger.info("ğŸ§  LLMì„ í†µí•œ ì‹¬ë³¼ ë¶„ì„ ì‹œì‘")
        
        for ast_data in all_ast_data:
            file_path = ast_data.get("file", {}).get("path", "unknown")
            symbols = ast_data.get("symbols", [])
            
            # íŒŒì¼ ì»¨í…ìŠ¤íŠ¸ (ì „ì²´ ì½”ë“œ ë‚´ìš©)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read()
                file_context = f"File: {file_path}, Language: {ast_data.get('file', {}).get('language', 'unknown')}\nContent:\n{file_content}"
            except Exception as e:
                logger.warning(f"âš ï¸  íŒŒì¼ ë‚´ìš© ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
                file_context = f"File: {file_path}, Language: {ast_data.get('file', {}).get('language', 'unknown')}"
            
            # íŒŒì¼ ë ˆë²¨ LLM ë¶„ì„
            try:
                file_analysis = self.llm_analyzer.analyze_file(ast_data, file_context)
                if "file" not in ast_data:
                    ast_data["file"] = {}
                ast_data["file"]["llm_analysis"] = file_analysis
                logger.info(f"âœ… íŒŒì¼ ë ˆë²¨ LLM ë¶„ì„ ì™„ë£Œ: {file_path}")
            except Exception as e:
                logger.error(f"âš ï¸  íŒŒì¼ ë ˆë²¨ LLM ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
            
            # ì‹¬ë³¼ë³„ LLM ë¶„ì„
            for symbol in symbols:
                try:
                    symbol_type = symbol.get("type", "")
                    
                    if symbol_type == "function":
                        # í•¨ìˆ˜/ë©”ì„œë“œ ìƒì„¸ ë¶„ì„ (ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©)
                        enriched_symbol = self.llm_analyzer.analyze_function(symbol, file_context)
                        symbol.update(enriched_symbol)
                        
                    elif symbol_type == "class":
                        # í´ë˜ìŠ¤ ìƒì„¸ ë¶„ì„ (ê¸°ì¡´ ë©”ì„œë“œ ì‚¬ìš©)
                        enriched_symbol = self.llm_analyzer.analyze_class(symbol, file_context)
                        symbol.update(enriched_symbol)
                        
                except Exception as e:
                    logger.error(f"âš ï¸  ì‹¬ë³¼ LLM ë¶„ì„ ì‹¤íŒ¨: {symbol.get('name', 'unknown')} in {file_path}: {e}")
        
        logger.info("âœ… LLM ì‹¬ë³¼ ë¶„ì„ ì™„ë£Œ")
        return all_ast_data

    def _save_to_vector_store(self, all_ast_data: List[Dict[str, Any]]) -> None:
        """ë²¡í„° ìŠ¤í† ì–´ì— ì‹¬ë³¼ ì €ì¥"""
        if not self.vector_store:
            return
        
        logger.info("ğŸ’¾ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹œì‘")
        
        total_symbols = 0
        for ast_data in all_ast_data:
            try:
                added_count = self.vector_store.add_file_symbols(ast_data)
                total_symbols += added_count
            except Exception as e:
                file_path = ast_data.get("file", {}).get("path", "unknown")
                logger.error(f"âš ï¸  ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨ {file_path}: {e}")
        
        # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        if self.vector_store.save():
            stats = self.vector_store.get_stats()
            total_vectors = stats.get('total_vectors', 0)
            unique_files = stats.get('unique_files', 0)
            logger.info(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {total_vectors}ê°œ ë²¡í„°, {unique_files}ê°œ íŒŒì¼")
        else:
            logger.error("âš ï¸  ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨")

    def _save_ast_json(self, all_ast_data: List[Dict[str, Any]]) -> None:
        """AST ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (LLM ë¶„ì„ ê²°ê³¼ í¬í•¨)"""
        try:
            # LLM ë¶„ì„ ê²°ê³¼ê°€ í¬í•¨ëœ ì™„ì „í•œ ë°ì´í„° ì €ì¥
            output_file = self.artifacts_dir / "structsynth_analysis_complete.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_ast_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ì™„ì „í•œ ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì™„ë£Œ: {output_file}")
            
            # ìš”ì•½ ì •ë³´ë„ ë³„ë„ë¡œ ì €ì¥
            summary_data = {
                "analysis_summary": {
                    "total_files": len(all_ast_data),
                    "total_symbols": sum(len(ast.get('symbols', [])) for ast in all_ast_data),
                    "files_with_llm_analysis": sum(1 for ast in all_ast_data if ast.get('file', {}).get('llm_analysis')),
                    "analysis_timestamp": str(datetime.now()),
                    "repo_path": str(self.repo_path)
                },
                "file_summaries": [
                    {
                        "file_path": ast.get('file', {}).get('path', 'unknown'),
                        "language": ast.get('file', {}).get('language', 'unknown'),
                        "symbols_count": len(ast.get('symbols', [])),
                        "has_llm_analysis": bool(ast.get('file', {}).get('llm_analysis')),
                        "llm_summary": ast.get('file', {}).get('llm_analysis', {}).get('summary', 'N/A')
                    }
                    for ast in all_ast_data
                ]
            }
            
            summary_file = self.artifacts_dir / "structsynth_analysis_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ë¶„ì„ ìš”ì•½ JSON ì €ì¥ ì™„ë£Œ: {summary_file}")
            
        except Exception as e:
            logger.error(f"âš ï¸  AST JSON ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_to_sqlite(self, all_ast_data: List[Dict[str, Any]]) -> None:
        """AST ë°ì´í„°ë¥¼ SQLiteì— ì €ì¥"""
        try:
            # SQLite ì €ì¥ ë¡œì§ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
            logger.info("âœ… SQLite ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âš ï¸  SQLite ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_to_faiss(self, all_ast_data: List[Dict[str, Any]]) -> None:
        """AST ë°ì´í„°ë¥¼ FAISSì— ì €ì¥"""
        try:
            # FAISS ì €ì¥ ë¡œì§ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
            logger.info("âœ… FAISS ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âš ï¸  FAISS ì €ì¥ ì‹¤íŒ¨: {e}")

    def search_symbols(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì‹¬ë³¼ ê²€ìƒ‰"""
        if not self.vector_store:
            logger.warning("âš ï¸  ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            results = self.vector_store.search(query, top_k)
            return results
        except Exception as e:
            logger.error(f"âš ï¸  ì‹¬ë³¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def get_vector_store_stats(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.vector_store:
            return {"error": "ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        try:
            return self.vector_store.get_stats()
        except Exception as e:
            logger.error(f"âš ï¸  ë²¡í„° ìŠ¤í† ì–´ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}