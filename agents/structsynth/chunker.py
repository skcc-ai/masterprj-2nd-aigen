"""
Code Chunker - AST ê¸°ë°˜ ì½”ë“œ ì²­í‚¹
ASTë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì½”ë“œë¥¼ ì˜ë¯¸ìˆëŠ” ì²­í¬ë¡œ ë¶„í• 
ì£¼ì„ ê²°í•© ë° í† í° ì œí•œì„ ê³ ë ¤í•œ ì •êµí•œ ì²­í‚¹
"""

import logging
import re
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ChunkConfig:
    """ì²­í‚¹ ì„¤ì •"""
    max_tokens: int = 1000
    min_tokens: int = 50
    overlap_tokens: int = 100
    include_comments: bool = True
    include_docstrings: bool = True
    max_comment_length: int = 200
    preserve_structure: bool = True


class CodeChunker:
    """AST ê¸°ë°˜ ì½”ë“œ ì²­í‚¹ (ì£¼ì„ ê²°í•© + í† í° ì œí•œ)"""

    def __init__(self, config: ChunkConfig = None):
        self.config = config or ChunkConfig()
        logger.info(f"CodeChunker initialized with config: {self.config}")

    def create_chunks(self, ast_result: Dict[str, Any], file_id: int) -> List[Dict[str, Any]]:
        """AST ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­í¬ ìƒì„±"""
        chunks = []
        
        # íŒŒì¼ ë ˆë²¨ ë©”íƒ€ë°ì´í„°
        file_info = ast_result.get("file", {})
        file_comments = self._extract_file_comments(ast_result)
        
        # ì‹¬ë³¼ë³„ ì²­í‚¹
        symbols = ast_result.get("symbols", [])
        for symbol in symbols:
            symbol_chunks = self._create_symbol_chunks(symbol, file_id, file_info, file_comments)
            chunks.extend(symbol_chunks)
        
        # íŒŒì¼ ë ˆë²¨ ìš”ì•½ ì²­í¬
        if file_comments:
            summary_chunk = self._create_summary_chunk(file_id, file_info, file_comments)
            chunks.append(summary_chunk)
        
        logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ (íŒŒì¼: {file_id})")
        return chunks

    def _create_symbol_chunks(self, symbol: Dict[str, Any], file_id: int, 
                            file_info: Dict[str, Any], file_comments: List[str]) -> List[Dict[str, Any]]:
        """ì‹¬ë³¼ë³„ ì²­í¬ ìƒì„±"""
        chunks = []
        
        # ì‹¬ë³¼ ì •ë³´ ì¶”ì¶œ
        symbol_name = symbol.get("name", "unknown")
        symbol_type = symbol.get("type", "unknown")
        symbol_body = symbol.get("body", {})
        
        # ì‹¬ë³¼ ê´€ë ¨ ì£¼ì„ ìˆ˜ì§‘
        symbol_comments = self._extract_symbol_comments(symbol, file_comments)
        
        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        body_content = self._extract_body_content(symbol_body)
        
        # ì£¼ì„ê³¼ ë³¸ë¬¸ ê²°í•©
        combined_content = self._combine_comments_and_body(symbol_comments, body_content)
        
        # í† í° ì œí•œì— ë”°ë¥¸ ì²­í‚¹
        if combined_content:
            content_chunks = self._split_by_tokens(combined_content, symbol_name)
            
            for i, chunk_content in enumerate(content_chunks):
                chunk = {
                    "file_id": file_id,
                    "symbol_id": f"{file_id}_{symbol_name}_{i}",
                    "chunk_type": f"{symbol_type}_chunk",
                    "symbol_name": symbol_name,
                    "symbol_type": symbol_type,
                    "content": chunk_content,
                    "line_start": symbol.get("location", {}).get("start_line", 0),
                    "line_end": symbol.get("location", {}).get("end_line", 0),
                    "tokens": self._count_tokens(chunk_content),
                    "chunk_index": i,
                    "total_chunks": len(content_chunks),
                    "metadata": {
                        "name": symbol_name,
                        "type": symbol_type,
                        "has_comments": bool(symbol_comments),
                        "comment_count": len(symbol_comments),
                        "preserves_structure": self.config.preserve_structure
                    }
                }
                chunks.append(chunk)
        
        return chunks

    def _extract_file_comments(self, ast_result: Dict[str, Any]) -> List[str]:
        """íŒŒì¼ ë ˆë²¨ ì£¼ì„ ì¶”ì¶œ"""
        comments = []
        
        # íŒŒì¼ ìƒë‹¨ ì£¼ì„
        if self.config.include_comments:
            file_comments = ast_result.get("comments", [])
            for comment in file_comments:
                comment_text = comment.get("text", "").strip()
                if comment_text and len(comment_text) <= self.config.max_comment_length:
                    comments.append(comment_text)
        
        # íŒŒì¼ docstring
        if self.config.include_docstrings:
            file_docstring = ast_result.get("file", {}).get("docstring", "")
            if file_docstring:
                comments.append(f"File Docstring: {file_docstring}")
        
        return comments

    def _extract_symbol_comments(self, symbol: Dict[str, Any], file_comments: List[str]) -> List[str]:
        """ì‹¬ë³¼ ê´€ë ¨ ì£¼ì„ ì¶”ì¶œ"""
        comments = []
        
        # ì‹¬ë³¼ ë°”ë¡œ ìœ„ì˜ ì£¼ì„
        symbol_comments = symbol.get("comments", [])
        for comment in symbol_comments:
            comment_text = comment.get("text", "").strip()
            if comment_text and len(comment_text) <= self.config.max_comment_length:
                comments.append(comment_text)
        
        # ì‹¬ë³¼ docstring
        symbol_docstring = symbol.get("docstring", "")
        if symbol_docstring:
            comments.append(f"Docstring: {symbol_docstring}")
        
        return comments

    def _extract_body_content(self, body: Dict[str, Any]) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        content_parts = []
        
        # ë…¸ë“œ ë‚´ìš©
        nodes = body.get("nodes", [])
        for node in nodes:
            node_content = node.get("content", "")
            if node_content:
                content_parts.append(node_content)
        
        # ê¸°íƒ€ ë‚´ìš©
        other_content = body.get("content", "")
        if other_content:
            content_parts.append(other_content)
        
        return "\n".join(content_parts)

    def _combine_comments_and_body(self, comments: List[str], body_content: str) -> str:
        """ì£¼ì„ê³¼ ë³¸ë¬¸ ê²°í•©"""
        combined = []
        
        # ì£¼ì„ ì¶”ê°€
        if comments:
            combined.append("## Comments and Documentation")
            for comment in comments:
                combined.append(f"# {comment}")
            combined.append("")
        
        # ë³¸ë¬¸ ì¶”ê°€
        if body_content:
            combined.append("## Code Implementation")
            combined.append(body_content)
        
        return "\n".join(combined)

    def _split_by_tokens(self, content: str, symbol_name: str) -> List[str]:
        """í† í° ì œí•œì— ë”°ë¥¸ ë‚´ìš© ë¶„í• """
        if not content:
            return []
        
        # í† í°ìœ¼ë¡œ ë¶„í• 
        tokens = content.split()
        chunks = []
        
        if len(tokens) <= self.config.max_tokens:
            # í•œ ë²ˆì— ì²˜ë¦¬ ê°€ëŠ¥
            chunks.append(content)
        else:
            # ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
            start = 0
            while start < len(tokens):
                end = min(start + self.config.max_tokens, len(tokens))
                
                # ì²­í¬ ìƒì„±
                chunk_tokens = tokens[start:end]
                chunk_content = " ".join(chunk_tokens)
                
                # ì²­í¬ê°€ ë„ˆë¬´ ì‘ì§€ ì•Šë„ë¡ ì¡°ì •
                if len(chunk_tokens) < self.config.min_tokens and start > 0:
                    # ì´ì „ ì²­í¬ì™€ ë³‘í•©
                    if chunks:
                        chunks[-1] += " " + chunk_content
                    else:
                        chunks.append(chunk_content)
                else:
                    chunks.append(chunk_content)
                
                # ì˜¤ë²„ë© ê³ ë ¤
                start = max(start + 1, end - self.config.overlap_tokens)
        
        logger.debug(f"ğŸ“ {symbol_name}: {len(tokens)} í† í°ì„ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        return chunks

    def _count_tokens(self, content: str) -> int:
        """í† í° ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ ê³µë°± ê¸°ì¤€)"""
        if not content:
            return 0
        return len(content.split())

    def _create_summary_chunk(self, file_id: int, file_info: Dict[str, Any], 
                            file_comments: List[str]) -> Dict[str, Any]:
        """íŒŒì¼ ìš”ì•½ ì²­í¬ ìƒì„±"""
        summary_content = "## File Summary\n"
        summary_content += f"File: {file_info.get('path', 'unknown')}\n"
        summary_content += f"Language: {file_info.get('language', 'unknown')}\n"
        summary_content += f"Total Comments: {len(file_comments)}\n\n"
        
        if file_comments:
            summary_content += "## Key Comments\n"
            for i, comment in enumerate(file_comments[:3], 1):  # ìƒìœ„ 3ê°œë§Œ
                summary_content += f"{i}. {comment}\n"
        
        return {
            "file_id": file_id,
            "symbol_id": f"{file_id}_summary",
            "chunk_type": "file_summary",
            "symbol_name": "file_summary",
            "symbol_type": "summary",
            "content": summary_content,
            "line_start": 1,
            "line_end": 1,
            "tokens": self._count_tokens(summary_content),
            "chunk_index": 0,
            "total_chunks": 1,
            "metadata": {
                "name": "file_summary",
                "type": "summary",
                "has_comments": bool(file_comments),
                "comment_count": len(file_comments),
                "preserves_structure": True
            }
        }

    def get_chunking_stats(self, chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì²­í‚¹ í†µê³„ ì •ë³´"""
        if not chunks:
            return {"error": "ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        total_tokens = sum(chunk.get("tokens", 0) for chunk in chunks)
        avg_tokens = total_tokens / len(chunks) if chunks else 0
        
        # ì²­í¬ íƒ€ì…ë³„ í†µê³„
        chunk_types = {}
        for chunk in chunks:
            chunk_type = chunk.get("chunk_type", "unknown")
            if chunk_type not in chunk_types:
                chunk_types[chunk_type] = {"count": 0, "total_tokens": 0}
            chunk_types[chunk_type]["count"] += 1
            chunk_types[chunk_type]["total_tokens"] += chunk.get("tokens", 0)
        
        return {
            "total_chunks": len(chunks),
            "total_tokens": total_tokens,
            "average_tokens_per_chunk": round(avg_tokens, 2),
            "chunk_types": chunk_types,
            "config": {
                "max_tokens": self.config.max_tokens,
                "min_tokens": self.config.min_tokens,
                "overlap_tokens": self.config.overlap_tokens,
                "include_comments": self.config.include_comments,
                "include_docstrings": self.config.include_docstrings
            }
        } 