"""
Vector Store - ì½”ë“œ ì‹¬ë³¼ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ìž¥
LLM ë¶„ì„ ê²°ê³¼ì™€ í•¨ê»˜ ê²€ìƒ‰ ê°€ëŠ¥í•œ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
"""

import logging
import json
import os
from typing import Dict, List, Any, Optional, Tuple
import openai
import numpy as np
from pathlib import Path

logger = logging.getLogger(__name__)


class VectorStore:
    """ì½”ë“œ ì‹¬ë³¼ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ìž¥í•˜ëŠ” ë²¡í„° ìŠ¤í† ì–´"""
    
    def __init__(self, api_key: Optional[str] = None, 
                 endpoint: Optional[str] = None,
                 embedding_deployment: str = "text-embedding-3-small",
                 storage_dir: str = "vector_store"):
        
        # Azure OpenAI ì„¤ì •
        self.api_key = api_key or os.getenv("AZURE_OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        self.endpoint = endpoint or os.getenv("AZURE_OPENAI_ENDPOINT")
        self.embedding_deployment = embedding_deployment or os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-3-small")
        
        if not self.api_key:
            raise RuntimeError("Azure OpenAI API key required")
        
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.client = openai.AzureOpenAI(
            api_key=self.api_key,
            api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
            azure_endpoint=self.endpoint
        )
        
        # ì €ìž¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        
        # ë²¡í„° ì €ìž¥ì†Œ ì´ˆê¸°í™”
        self.vectors: List[Tuple[List[float], Dict[str, Any]]] = []
        self.metadata_file = self.storage_dir / "metadata.json"
        self.vectors_file = self.storage_dir / "vectors.npy"
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        self._load_existing_data()
        
        logger.info(f"VectorStore initialized with Azure OpenAI deployment: {self.embedding_deployment}")

    def add_symbol(self, symbol_data: Dict[str, Any], file_path: str) -> bool:
        """ì‹¬ë³¼ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ìž¥"""
        try:
            # ì‹¬ë³¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            text_content = self._symbol_to_text(symbol_data, file_path)
            
            # ìž„ë² ë”© ìƒì„±
            embedding = self._create_embedding(text_content)
            if not embedding:
                return False
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                "file_path": file_path,
                "symbol_type": symbol_data.get("type", "unknown"),
                "symbol_name": symbol_data.get("name", "unknown"),
                "location": symbol_data.get("location", {}),
                "llm_analysis": symbol_data.get("llm_analysis", {}),
                "text_content": text_content[:500],  # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ (ì•žë¶€ë¶„ë§Œ)
                "timestamp": self._get_timestamp()
            }
            
            # ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„° ì €ìž¥
            self.vectors.append((embedding, metadata))
            
            logger.info(f"âœ… ì‹¬ë³¼ ë²¡í„° ì €ìž¥ ì™„ë£Œ: {symbol_data.get('name', 'unknown')} in {file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âš ï¸ ì‹¬ë³¼ ë²¡í„° ì €ìž¥ ì‹¤íŒ¨: {e}")
            return False

    def add_file_symbols(self, ast_data: Dict[str, Any]) -> int:
        """íŒŒì¼ì˜ ëª¨ë“  ì‹¬ë³¼ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ìž¥"""
        file_path = ast_data.get("file", {}).get("path", "unknown")
        symbols = ast_data.get("symbols", [])
        
        added_count = 0
        for symbol in symbols:
            if self.add_symbol(symbol, file_path):
                added_count += 1
        
        logger.info(f"âœ… {file_path}: {added_count}/{len(symbols)}ê°œ ì‹¬ë³¼ ë²¡í„° ì €ìž¥ ì™„ë£Œ")
        return added_count

    def add_chunk(self, chunk_data: Dict[str, Any]) -> bool:
        """ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ìž¥"""
        try:
            # ì²­í¬ ë°ì´í„°ë¥¼ ì‹¬ë³¼ í˜•íƒœë¡œ ë³€í™˜
            symbol_data = {
                "name": chunk_data.get("symbol_name", "unknown"),
                "type": chunk_data.get("symbol_type", "chunk"),
                "content": chunk_data.get("content", ""),
                "location": chunk_data.get("location", {}),
                "metadata": chunk_data.get("metadata", {})
            }
            
            file_path = chunk_data.get("file_path", "unknown")
            
            # add_symbol ë©”ì„œë“œ ì‚¬ìš©
            return self.add_symbol(symbol_data, file_path)
            
        except Exception as e:
            logger.error(f"âš ï¸ ì²­í¬ ë²¡í„° ì €ìž¥ ì‹¤íŒ¨: {e}")
            return False

    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ì‹¬ë³¼ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ìž„ë² ë”© ìƒì„±
            query_embedding = self._create_embedding(query)
            if not query_embedding:
                return []
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for vector, metadata in self.vectors:
                similarity = self._cosine_similarity(query_embedding, vector)
                similarities.append((similarity, metadata))
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarities.sort(key=lambda x: x[0], reverse=True)
            
            # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
            results = []
            for similarity, metadata in similarities[:top_k]:
                result = metadata.copy()
                result["similarity_score"] = float(similarity)
                results.append(result)
            
            logger.info(f"ðŸ” ê²€ìƒ‰ ì™„ë£Œ: '{query}' -> {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def save(self) -> bool:
        """ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì¼ì— ì €ìž¥"""
        try:
            # ë©”íƒ€ë°ì´í„° ì €ìž¥
            metadata_list = [metadata for _, metadata in self.vectors]
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_list, f, ensure_ascii=False, indent=2)
            
            # ë²¡í„° ì €ìž¥
            vectors_array = np.array([vector for vector, _ in self.vectors])
            np.save(self.vectors_file, vectors_array)
            
            logger.info(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ìž¥ ì™„ë£Œ: {len(self.vectors)}ê°œ ë²¡í„°")
            return True
            
        except Exception as e:
            logger.error(f"âš ï¸ ë²¡í„° ìŠ¤í† ì–´ ì €ìž¥ ì‹¤íŒ¨: {e}")
            return False

    def load(self) -> bool:
        """ì €ìž¥ëœ ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            if not self.metadata_file.exists() or not self.vectors_file.exists():
                logger.info("ì €ìž¥ëœ ë²¡í„° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                metadata_list = json.load(f)
            
            # ë²¡í„° ë¡œë“œ
            vectors_array = np.load(self.vectors_file)
            
            # ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„° ê²°í•©
            self.vectors = list(zip(vectors_array, metadata_list))
            
            logger.info(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ: {len(self.vectors)}ê°œ ë²¡í„°")
            return True
            
        except Exception as e:
            logger.error(f"âš ï¸ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ í†µê³„ ì •ë³´"""
        if not self.vectors:
            return {"total_vectors": 0}
        
        # íŒŒì¼ë³„ ì‹¬ë³¼ ìˆ˜
        file_counts = {}
        symbol_type_counts = {}
        
        for _, metadata in self.vectors:
            file_path = metadata.get("file_path", "unknown")
            symbol_type = metadata.get("symbol_type", "unknown")
            
            file_counts[file_path] = file_counts.get(file_path, 0) + 1
            symbol_type_counts[symbol_type] = symbol_type_counts.get(symbol_type, 0) + 1
        
        return {
            "total_vectors": len(self.vectors),
            "unique_files": len(file_counts),
            "file_counts": file_counts,
            "symbol_type_counts": symbol_type_counts
        }

    def _symbol_to_text(self, symbol_data: Dict[str, Any], file_path: str) -> str:
        """ì‹¬ë³¼ ë°ì´í„°ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text_parts = []
        
        # ê¸°ë³¸ ì •ë³´
        text_parts.append(f"Symbol: {symbol_data.get('name', 'unknown')}")
        text_parts.append(f"Type: {symbol_data.get('type', 'unknown')}")
        text_parts.append(f"File: {file_path}")
        
        # ì‹œê·¸ë‹ˆì²˜
        if symbol_data.get('signature'):
            text_parts.append(f"Signature: {symbol_data['signature']}")
        
        # ë©”íƒ€ë°ì´í„°
        metadata = symbol_data.get('metadata', {})
        if metadata.get('parameters'):
            text_parts.append(f"Parameters: {', '.join(metadata['parameters'])}")
        if metadata.get('return_type'):
            text_parts.append(f"Return type: {metadata['return_type']}")
        if metadata.get('bases'):
            text_parts.append(f"Bases: {', '.join(metadata['bases'])}")
        
        # LLM ë¶„ì„ ê²°ê³¼
        llm_analysis = symbol_data.get('llm_analysis', {})
        if llm_analysis.get('purpose'):
            text_parts.append(f"Purpose: {llm_analysis['purpose']}")
        if llm_analysis.get('summary'):
            text_parts.append(f"Summary: {llm_analysis['summary']}")
        if llm_analysis.get('dependencies'):
            text_parts.append(f"Dependencies: {', '.join(llm_analysis['dependencies'])}")
        if llm_analysis.get('key_operations'):
            text_parts.append(f"Key operations: {', '.join(llm_analysis['key_operations'])}")
        
        # ë³¸ë¬¸ ë‚´ìš© (ê°„ë‹¨í•œ ìš”ì•½)
        body = symbol_data.get('body', {})
        if body.get('nodes'):
            node_types = [node.get('type', 'unknown') for node in body['nodes'][:5]]  # ì²˜ìŒ 5ê°œë§Œ
            text_parts.append(f"Body nodes: {', '.join(node_types)}")
        
        return " | ".join(text_parts)

    def _create_embedding(self, text: str) -> Optional[List[float]]:
        """í…ìŠ¤íŠ¸ë¥¼ ìž„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.embedding_deployment
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"ìž„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            vec1_array = np.array(vec1)
            vec2_array = np.array(vec2)
            
            dot_product = np.dot(vec1_array, vec2_array)
            norm1 = np.linalg.norm(vec1_array)
            norm2 = np.linalg.norm(vec2_array)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return dot_product / (norm1 * norm2)
        except Exception:
            return 0.0

    def _load_existing_data(self):
        """ê¸°ì¡´ ì €ìž¥ ë°ì´í„° ë¡œë“œ"""
        try:
            self.load()
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _get_timestamp(self) -> str:
        """í˜„ìž¬ íƒ€ìž„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        from datetime import datetime
        return datetime.now().isoformat()
