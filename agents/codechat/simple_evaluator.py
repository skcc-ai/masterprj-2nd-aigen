"""
ê°„ë‹¨í•œ AI ììœ¨ í‰ê°€ ì‹œìŠ¤í…œ - ì§ˆë¬¸ê³¼ ë‹µë³€ë§Œìœ¼ë¡œ í‰ê°€
"""

import logging
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
import json
import re

logger = logging.getLogger(__name__)

@dataclass
class SimpleEvaluation:
    """ê°„ë‹¨í•œ í‰ê°€ ê²°ê³¼"""
    score: float  # 0-100 ì ìˆ˜
    is_sufficient: bool  # ì¶©ë¶„í•œ ë‹µë³€ì¸ì§€
    missing_parts: List[str]  # ë¶€ì¡±í•œ ë¶€ë¶„ë“¤
    feedback: str  # ê°œì„  í”¼ë“œë°±
    confidence: float  # í‰ê°€ ì‹ ë¢°ë„

class SimpleSelfEvaluator:
    """ê°„ë‹¨í•œ AI ììœ¨ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, openai_client, deployment_name: str):
        self.openai_client = openai_client
        self.deployment_name = deployment_name
        self.min_score = 70  # ìµœì†Œ í†µê³¼ ì ìˆ˜
        
    def evaluate_answer(self, question: str, answer: str, context: Optional[Dict[str, Any]] = None) -> SimpleEvaluation:
        """ì§ˆë¬¸ê³¼ ë‹µë³€ë§Œìœ¼ë¡œ í‰ê°€"""
        
        try:
            logger.info(f"ğŸ” AI ììœ¨ í‰ê°€ ì‹œì‘: '{question[:50]}...'")
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„
            context_info = ""
            if context and context.get("recent_conversations"):
                recent_queries = [conv["query"] for conv in context["recent_conversations"][-2:]]
                context_info = f"ìµœê·¼ ëŒ€í™” ë§¥ë½: {', '.join(recent_queries)}"
            
            # í‰ê°€ í”„ë¡¬í”„íŠ¸
            evaluation_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì ì ˆí•œì§€ í‰ê°€í•˜ì„¸ìš”:

**ì§ˆë¬¸**: {question}

**ë‹µë³€**: {answer}

**ë§¥ë½**: {context_info}

**í‰ê°€ ê¸°ì¤€**:
1. ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í–ˆëŠ”ê°€?
2. ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•œê°€?
3. ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í–ˆëŠ”ê°€?
4. í•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€?
5. ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í–ˆëŠ”ê°€?
6. í˜„ì¬ ì½”ë“œë² ì´ìŠ¤ì˜ ì‹¤ì œ ì½”ë“œë¥¼ ë¶„ì„í–ˆëŠ”ê°€?
7. êµ¬ì²´ì ì¸ íŒŒì¼ëª…, í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª…ì„ ì–¸ê¸‰í–ˆëŠ”ê°€?

**í‰ê°€ ë°©ë²•**:
- 0-100ì ìœ¼ë¡œ ì ìˆ˜ ë§¤ê¸°ê¸°
- 70ì  ì´ìƒì´ë©´ ì¶©ë¶„í•œ ë‹µë³€
- ë¶€ì¡±í•œ ë¶€ë¶„ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì 
- ê°œì„  ë°©í–¥ ì œì‹œ

**íŠ¹ë³„ ì£¼ì˜ì‚¬í•­**:
- ì½”ë“œ ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš° í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ ì½”ë“œë¥¼ ë¶„ì„í–ˆëŠ”ì§€ í™•ì¸
- ì¼ë°˜ë¡ ë§Œ ì œê³µí•˜ê³  êµ¬ì²´ì ì¸ íŒŒì¼/í•¨ìˆ˜/í´ë˜ìŠ¤ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ ë‚®ì€ ì ìˆ˜

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "score": 85,
    "is_sufficient": true,
    "missing_parts": ["êµ¬ì²´ì ì¸ ì‚¬ìš© ì˜ˆì‹œ", "ì„±ëŠ¥ íŠ¹ì„±"],
    "feedback": "ë‹µë³€ì´ ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì§€ë§Œ, ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œì™€ ì„±ëŠ¥ íŠ¹ì„±ì„ ì¶”ê°€í•˜ë©´ ë” ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
    "confidence": 0.9
}}
"""
            
            # LLM í‰ê°€ ì‹¤í–‰
            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì½”ë“œ ë¶„ì„ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê°ê´€ì ì´ê³  ì •í™•í•œ í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.1,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
                max_tokens=500
            )
            
            llm_response = response.choices[0].message.content
            evaluation = self._parse_evaluation(llm_response)
            
            logger.info(f"ğŸ“Š í‰ê°€ ê²°ê³¼: {evaluation.score:.1f}/100 (ì¶©ë¶„í•¨: {evaluation.is_sufficient})")
            if evaluation.missing_parts:
                logger.info(f"   - ë¶€ì¡±í•œ ë¶€ë¶„: {', '.join(evaluation.missing_parts)}")
            
            return evaluation
            
        except Exception as e:
            logger.error(f"í‰ê°€ ì‹¤íŒ¨: {e}")
            return self._create_default_evaluation()
    
    def _parse_evaluation(self, llm_response: str) -> SimpleEvaluation:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ í‰ê°€ ê²°ê³¼ ìƒì„±"""
        
        try:
            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
            if json_match:
                eval_data = json.loads(json_match.group())
                
                return SimpleEvaluation(
                    score=float(eval_data.get("score", 0)),
                    is_sufficient=bool(eval_data.get("is_sufficient", False)),
                    missing_parts=eval_data.get("missing_parts", []),
                    feedback=eval_data.get("feedback", ""),
                    confidence=float(eval_data.get("confidence", 0.5))
                )
            else:
                logger.warning("í‰ê°€ ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return self._create_default_evaluation()
                
        except Exception as e:
            logger.error(f"í‰ê°€ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return self._create_default_evaluation()
    
    def _create_default_evaluation(self) -> SimpleEvaluation:
        """ê¸°ë³¸ í‰ê°€ ê²°ê³¼ ìƒì„±"""
        return SimpleEvaluation(
            score=50,
            is_sufficient=False,
            missing_parts=["í‰ê°€ ì‹¤íŒ¨"],
            feedback="í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            confidence=0.1
        )
    
    def improve_answer(self, question: str, current_answer: str, evaluation: SimpleEvaluation, 
                      context: Optional[Dict[str, Any]] = None, codechat_agent=None) -> str:
        """ë¶€ì¡±í•œ ë¶€ë¶„ì„ í¬í•¨í•˜ì—¬ ë‹µë³€ ê°œì„ """
        
        if evaluation.is_sufficient:
            logger.info("âœ… ë‹µë³€ì´ ì¶©ë¶„í•©ë‹ˆë‹¤. ê°œì„  ë¶ˆí•„ìš”.")
            return current_answer
        
        try:
            logger.info(f"ğŸ”§ ë‹µë³€ ê°œì„  ì‹œì‘: {evaluation.missing_parts}")
            
            # 1. ë¶€ì¡±í•œ ë¶€ë¶„ì— ëŒ€í•œ ì¶”ê°€ ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½
            search_plan = self._create_search_plan(question, evaluation, context)
            
            # 2. ì‹¤ì œ íˆ´ì„ ì‚¬ìš©í•œ ì¶”ê°€ ê²€ìƒ‰ (CodeChatAgentê°€ ì œê³µëœ ê²½ìš°)
            additional_info = ""
            if codechat_agent and search_plan:
                additional_info = self._execute_additional_search(
                    codechat_agent, search_plan, question, context
                )
            
            # 3. ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„
            context_info = ""
            if context and context.get("recent_conversations"):
                recent_queries = [conv["query"] for conv in context["recent_conversations"][-2:]]
                context_info = f"ìµœê·¼ ëŒ€í™” ë§¥ë½: {', '.join(recent_queries)}"
            
            # 4. ê°œì„  í”„ë¡¬í”„íŠ¸ (ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨)
            improvement_prompt = f"""
ë‹¤ìŒ ë‹µë³€ì„ ê°œì„ í•˜ì„¸ìš”:

**ì§ˆë¬¸**: {question}

**í˜„ì¬ ë‹µë³€**: {current_answer}

**ë¶€ì¡±í•œ ë¶€ë¶„**: {', '.join(evaluation.missing_parts)}

**ê°œì„  í”¼ë“œë°±**: {evaluation.feedback}

**ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼**: {additional_info if additional_info else "ì¶”ê°€ ê²€ìƒ‰ ì—†ìŒ"}

**ë§¥ë½**: {context_info}

**ê°œì„  ì›ì¹™**:
1. ê¸°ì¡´ ë‹µë³€ì˜ êµ¬ì¡°ì™€ íë¦„ì„ ìœ ì§€
2. ë¶€ì¡±í•œ ë¶€ë¶„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€
3. ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ë‚´ìš© í¬í•¨
4. ì´ì „ ëŒ€í™” ë§¥ë½ ê³ ë ¤
5. ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ ì‚¬ìš©

ê°œì„ ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:
"""
            
            # 5. LLM ê°œì„  ì‹¤í–‰
            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹µë³€ì„ ê°œì„ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¶€ì¡±í•œ ë¶€ë¶„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•˜ì—¬ ë” ì™„ì „í•œ ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": improvement_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            improved_answer = response.choices[0].message.content
            logger.info("âœ… ë‹µë³€ ê°œì„  ì™„ë£Œ")
            return improved_answer
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ê°œì„  ì‹¤íŒ¨: {e}")
            return current_answer
    
    def _create_search_plan(self, question: str, evaluation: SimpleEvaluation, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, str]]:
        """ë¶€ì¡±í•œ ë¶€ë¶„ì— ëŒ€í•œ ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½"""
        
        try:
            search_plan_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì˜ ë‹µë³€ì—ì„œ ë¶€ì¡±í•œ ë¶€ë¶„ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ê²€ìƒ‰ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:

**ì§ˆë¬¸**: {question}

**ë¶€ì¡±í•œ ë¶€ë¶„**: {', '.join(evaluation.missing_parts)}

**ê°œì„  í”¼ë“œë°±**: {evaluation.feedback}

**ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë“¤**:
- search_symbols_fts: í‚¤ì›Œë“œ ê²€ìƒ‰ (íŠ¹ì • í‚¤ì›Œë“œë¡œ ì½”ë“œ ê²€ìƒ‰)
- search_symbols_semantic: ì˜ë¯¸ ê²€ìƒ‰ (ì˜ë¯¸ ìœ ì‚¬ë„ë¡œ ì½”ë“œ ê²€ìƒ‰)
- get_symbol: ì‹¬ë³¼ ìƒì„¸ ì •ë³´ (íŠ¹ì • ì‹¬ë³¼ì˜ ìƒì„¸ ì •ë³´)
- get_calls_from: í˜¸ì¶œ ê´€ê³„ (ì–´ë–¤ í•¨ìˆ˜ë“¤ì´ í˜¸ì¶œí•˜ëŠ”ì§€)
- get_calls_to: í˜¸ì¶œë°›ëŠ” ê´€ê³„ (ì–´ë–¤ í•¨ìˆ˜ë“¤ì´ í˜¸ì¶œí•˜ëŠ”ì§€)
- analyze_symbol_llm: LLM ì‹¬ë³¼ ë¶„ì„ (ì‹¬ë³¼ì„ LLMìœ¼ë¡œ ë¶„ì„)
- analyze_file_llm: LLM íŒŒì¼ ë¶„ì„ (íŒŒì¼ì„ LLMìœ¼ë¡œ ë¶„ì„)
- analyze_chunk_llm: LLM ì²­í¬ ë¶„ì„ (ì½”ë“œ ì²­í¬ë¥¼ LLMìœ¼ë¡œ ë¶„ì„)
- get_artifact: ì‚°ì¶œë¬¼ ì¡°íšŒ (InsightGen ìƒì„± ë¬¸ì„œ ì¡°íšŒ)
- list_artifacts: ì‚°ì¶œë¬¼ ëª©ë¡ (ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ëª©ë¡)
- get_artifact_summary: ì‚°ì¶œë¬¼ ìš”ì•½ (ë¬¸ì„œ ìš”ì•½ ì •ë³´)

ë¶€ì¡±í•œ ë¶€ë¶„ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë¬´ì—‡ì„ ê²€ìƒ‰í•´ì•¼ í•˜ëŠ”ì§€ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:

{{
    "searches": [
        {{"tool": "search_symbols_fts", "query": "ì‚¬ìš© ì˜ˆì‹œ", "reason": "êµ¬ì²´ì ì¸ ì‚¬ìš© ì˜ˆì‹œ ì°¾ê¸°"}},
        {{"tool": "analyze_symbol_llm", "target": "ì„±ëŠ¥", "reason": "ì„±ëŠ¥ íŠ¹ì„± ë¶„ì„"}}
    ]
}}
"""
            
            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¶€ì¡±í•œ ë¶€ë¶„ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì „ëµì„ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": search_plan_prompt}
                ],
                temperature=0.3,
                max_tokens=400
            )
            
            llm_response = response.choices[0].message.content
            return self._parse_search_plan(llm_response)
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_search_plan(self, llm_response: str) -> List[Dict[str, str]]:
        """ê²€ìƒ‰ ê³„íš íŒŒì‹±"""
        
        try:
            import re
            json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
            if json_match:
                plan_data = json.loads(json_match.group())
                return plan_data.get("searches", [])
            else:
                return []
                
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ê³„íš íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _execute_additional_search(self, codechat_agent, search_plan: List[Dict[str, str]], 
                                 question: str, context: Optional[Dict[str, Any]] = None) -> str:
        """AIê°€ ì„ íƒí•œ íˆ´ë¡œ ì¶”ê°€ ê²€ìƒ‰ ì‹¤í–‰"""
        
        try:
            search_results = []
            
            for search in search_plan:
                tool = search.get("tool")
                query = search.get("query", question)
                reason = search.get("reason", "ê°œì„ ì„ ìœ„í•œ ì¶”ê°€ ê²€ìƒ‰")
                
                logger.info(f"ğŸ” ì¶”ê°€ ê²€ìƒ‰ ì‹¤í–‰: {tool} - {query} ({reason})")
                
                # AIê°€ ì„ íƒí•œ íˆ´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì‹¤í–‰
                result = self._execute_single_search(codechat_agent, tool, query, reason)
                if result:
                    search_results.append(result)
            
            return "\n".join(search_results) if search_results else "ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
            
        except Exception as e:
            logger.error(f"ì¶”ê°€ ê²€ìƒ‰ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return "ì¶”ê°€ ê²€ìƒ‰ ì‹¤íŒ¨"
    
    def _execute_single_search(self, codechat_agent, tool: str, query: str, reason: str) -> str:
        """ë‹¨ì¼ ê²€ìƒ‰ ì‹¤í–‰ - AIê°€ ì„ íƒí•œ íˆ´ì— ë”°ë¼ ë™ì  ì‹¤í–‰"""
        
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ íˆ´ë“¤ì„ ë™ì ìœ¼ë¡œ ë§¤í•‘
            tool_mapping = {
                "search_symbols_fts": self._execute_fts_search,
                "search_symbols_semantic": self._execute_semantic_search,
                "get_symbol": self._execute_get_symbol,
                "get_calls_from": self._execute_get_calls_from,
                "get_calls_to": self._execute_get_calls_to,
                "analyze_symbol_llm": self._execute_analyze_symbol_llm,
                "analyze_file_llm": self._execute_analyze_file_llm,
                "analyze_chunk_llm": self._execute_analyze_chunk_llm,
                "get_artifact": self._execute_get_artifact,
                "list_artifacts": self._execute_list_artifacts,
                "get_artifact_summary": self._execute_get_artifact_summary
            }
            
            # AIê°€ ì„ íƒí•œ íˆ´ ì‹¤í–‰
            if tool in tool_mapping:
                return tool_mapping[tool](codechat_agent, query, reason)
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” íˆ´: {tool}")
                return f"ì•Œ ìˆ˜ ì—†ëŠ” íˆ´: {tool}"
                
        except Exception as e:
            logger.error(f"íˆ´ ì‹¤í–‰ ì‹¤íŒ¨ ({tool}): {e}")
            return f"íˆ´ ì‹¤í–‰ ì‹¤íŒ¨: {tool}"
    
    def _execute_fts_search(self, codechat_agent, query: str, reason: str) -> str:
        """FTS ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            results = codechat_agent._fts_search(query, 3)
            if results:
                return f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ({query}): {len(results)}ê°œ ë°œê²¬ - {reason}"
            else:
                return f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query}"
        except Exception as e:
            return f"FTS ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _execute_semantic_search(self, codechat_agent, query: str, reason: str) -> str:
        """ì˜ë¯¸ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            results = codechat_agent._faiss_search(query, 3)
            if results:
                return f"ì˜ë¯¸ ê²€ìƒ‰ ê²°ê³¼ ({query}): {len(results)}ê°œ ë°œê²¬ - {reason}"
            else:
                return f"ì˜ë¯¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query}"
        except Exception as e:
            return f"ì˜ë¯¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}"
    
    def _execute_get_symbol(self, codechat_agent, query: str, reason: str) -> str:
        """ì‹¬ë³¼ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì‹¬ë³¼ëª… ì¶”ì¶œ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            symbol_name = query.split()[0] if query.split() else query
            result = codechat_agent.sqlite_store.get_symbol(symbol_name)
            if result:
                return f"ì‹¬ë³¼ ì •ë³´ ({symbol_name}): {result.get('symbol_type', 'unknown')} - {reason}"
            else:
                return f"ì‹¬ë³¼ ì •ë³´ ì—†ìŒ: {symbol_name}"
        except Exception as e:
            return f"ì‹¬ë³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}"
    
    def _execute_get_calls_from(self, codechat_agent, query: str, reason: str) -> str:
        """í˜¸ì¶œ ê´€ê³„ ì¡°íšŒ (from)"""
        try:
            symbol_name = query.split()[0] if query.split() else query
            results = codechat_agent.sqlite_store.get_calls_from(symbol_name)
            if results:
                return f"í˜¸ì¶œ ê´€ê³„ ({symbol_name}): {len(results)}ê°œ í˜¸ì¶œ - {reason}"
            else:
                return f"í˜¸ì¶œ ê´€ê³„ ì—†ìŒ: {symbol_name}"
        except Exception as e:
            return f"í˜¸ì¶œ ê´€ê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}"
    
    def _execute_get_calls_to(self, codechat_agent, query: str, reason: str) -> str:
        """í˜¸ì¶œ ê´€ê³„ ì¡°íšŒ (to)"""
        try:
            symbol_name = query.split()[0] if query.split() else query
            results = codechat_agent.sqlite_store.get_calls_to(symbol_name)
            if results:
                return f"í˜¸ì¶œë°›ëŠ” ê´€ê³„ ({symbol_name}): {len(results)}ê°œ í˜¸ì¶œ - {reason}"
            else:
                return f"í˜¸ì¶œë°›ëŠ” ê´€ê³„ ì—†ìŒ: {symbol_name}"
        except Exception as e:
            return f"í˜¸ì¶œë°›ëŠ” ê´€ê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}"
    
    def _execute_analyze_symbol_llm(self, codechat_agent, query: str, reason: str) -> str:
        """LLM ì‹¬ë³¼ ë¶„ì„"""
        try:
            # ê°„ë‹¨í•œ LLM ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
            return f"LLM ì‹¬ë³¼ ë¶„ì„ ê²°ê³¼: {query}ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ - {reason}"
        except Exception as e:
            return f"LLM ì‹¬ë³¼ ë¶„ì„ ì‹¤íŒ¨: {e}"
    
    def _execute_analyze_file_llm(self, codechat_agent, query: str, reason: str) -> str:
        """LLM íŒŒì¼ ë¶„ì„"""
        try:
            return f"LLM íŒŒì¼ ë¶„ì„ ê²°ê³¼: {query}ì— ëŒ€í•œ íŒŒì¼ ë¶„ì„ - {reason}"
        except Exception as e:
            return f"LLM íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨: {e}"
    
    def _execute_analyze_chunk_llm(self, codechat_agent, query: str, reason: str) -> str:
        """LLM ì²­í¬ ë¶„ì„"""
        try:
            return f"LLM ì²­í¬ ë¶„ì„ ê²°ê³¼: {query}ì— ëŒ€í•œ ì²­í¬ ë¶„ì„ - {reason}"
        except Exception as e:
            return f"LLM ì²­í¬ ë¶„ì„ ì‹¤íŒ¨: {e}"
    
    def _execute_get_artifact(self, codechat_agent, query: str, reason: str) -> str:
        """ì‚°ì¶œë¬¼ ì¡°íšŒ"""
        try:
            # InsightGen ë„êµ¬ ì‚¬ìš© (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            return f"ì‚°ì¶œë¬¼ ì¡°íšŒ ê²°ê³¼: {query} ê´€ë ¨ ë¬¸ì„œ - {reason}"
        except Exception as e:
            return f"ì‚°ì¶œë¬¼ ì¡°íšŒ ì‹¤íŒ¨: {e}"
    
    def _execute_list_artifacts(self, codechat_agent, query: str, reason: str) -> str:
        """ì‚°ì¶œë¬¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            return f"ì‚°ì¶œë¬¼ ëª©ë¡: {query} ê´€ë ¨ ë¬¸ì„œë“¤ - {reason}"
        except Exception as e:
            return f"ì‚°ì¶œë¬¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}"
    
    def _execute_get_artifact_summary(self, codechat_agent, query: str, reason: str) -> str:
        """ì‚°ì¶œë¬¼ ìš”ì•½ ì¡°íšŒ"""
        try:
            return f"ì‚°ì¶œë¬¼ ìš”ì•½: {query} ê´€ë ¨ ë¬¸ì„œ ìš”ì•½ - {reason}"
        except Exception as e:
            return f"ì‚°ì¶œë¬¼ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}"
    
    def evaluate_and_improve(self, question: str, answer: str, context: Optional[Dict[str, Any]] = None, codechat_agent=None) -> Tuple[str, SimpleEvaluation]:
        """í‰ê°€í•˜ê³  í•„ìš”ì‹œ ê°œì„ """
        
        # 1. í‰ê°€
        evaluation = self.evaluate_answer(question, answer, context)
        
        # 2. ê°œì„  (í•„ìš”ì‹œ)
        if not evaluation.is_sufficient:
            improved_answer = self.improve_answer(question, answer, evaluation, context, codechat_agent)
            return improved_answer, evaluation
        else:
            return answer, evaluation
