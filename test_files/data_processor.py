"""
Data Processor - ë°ì´í„° ì²˜ë¦¬ ë° ë³€í™˜ ì—”ì§„
ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì½ê³ , ê²€ì¦í•˜ê³ , ë³€í™˜í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤
"""

import json
import csv
import logging
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
import re
import statistics
from collections import defaultdict, Counter
import threading
import queue
import time

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.processed_data = []
        self.error_log = []
        self.validation_rules = self._load_validation_rules()
        self.transformation_pipeline = self._setup_transformation_pipeline()
        self.data_cache = {}
        self.processing_stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'start_time': datetime.now(),
            'last_processed': None
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•œ ë½
        self._lock = threading.Lock()
        self._processing_queue = queue.Queue()
        
        logger.info("DataProcessor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_validation_rules(self) -> Dict[str, Any]:
        """ë°ì´í„° ê²€ì¦ ê·œì¹™ ë¡œë“œ"""
        return {
            'required_fields': ['id', 'name', 'timestamp'],
            'field_types': {
                'id': str,
                'name': str,
                'timestamp': str,
                'value': (int, float),
                'category': str,
                'tags': list
            },
            'field_constraints': {
                'name': {'min_length': 1, 'max_length': 100},
                'value': {'min': 0, 'max': 1000000},
                'category': {'allowed_values': ['A', 'B', 'C', 'D']}
            },
            'custom_validators': {
                'timestamp': self._validate_timestamp,
                'email': self._validate_email,
                'phone': self._validate_phone
            }
        }
    
    def _setup_transformation_pipeline(self) -> List[callable]:
        """ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        return [
            self._normalize_strings,
            self._convert_data_types,
            self._add_derived_fields,
            self._clean_data,
            self._enrich_data
        ]
    
    def process_data_file(self, file_path: str, file_type: str = None) -> Dict[str, Any]:
        """ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        try:
            logger.info(f"ğŸ”„ ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path}")
            
            # íŒŒì¼ íƒ€ì… ìë™ ê°ì§€
            if not file_type:
                file_type = self._detect_file_type(file_path)
            
            # íŒŒì¼ ì½ê¸°
            raw_data = self._read_file(file_path, file_type)
            
            # ë°ì´í„° ê²€ì¦
            validated_data = self._validate_data(raw_data)
            
            # ë°ì´í„° ë³€í™˜
            transformed_data = self._transform_data(validated_data)
            
            # ê²°ê³¼ ì €ì¥
            with self._lock:
                self.processed_data.extend(transformed_data)
                self.processing_stats['total_processed'] += len(transformed_data)
                self.processing_stats['successful'] += len(transformed_data)
                self.processing_stats['last_processed'] = datetime.now()
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            self._update_cache(file_path, transformed_data)
            
            result = {
                'success': True,
                'file_path': file_path,
                'file_type': file_type,
                'records_processed': len(transformed_data),
                'processing_time': datetime.now().isoformat()
            }
            
            logger.info(f" ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path} - {len(transformed_data)}ê°œ ë ˆì½”ë“œ")
            return result
            
        except Exception as e:
            error_msg = f"ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}"
            self.error_log.append(error_msg)
            logger.error(error_msg)
            
            with self._lock:
                self.processing_stats['failed'] += 1
            
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e),
                'processing_time': datetime.now().isoformat()
            }
    
    def _detect_file_type(self, file_path: str) -> str:
        """íŒŒì¼ íƒ€ì… ìë™ ê°ì§€"""
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension == '.json':
            return 'json'
        elif file_extension == '.csv':
            return 'csv'
        elif file_extension == '.txt':
            return 'text'
        elif file_extension in ['.xlsx', '.xls']:
            return 'excel'
        else:
            return 'unknown'
    
    def _read_file(self, file_path: str, file_type: str) -> List[Dict[str, Any]]:
        """íŒŒì¼ ì½ê¸°"""
        try:
            if file_type == 'json':
                return self._read_json_file(file_path)
            elif file_type == 'csv':
                return self._read_csv_file(file_path)
            elif file_type == 'text':
                return self._read_text_file(file_path)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…: {file_type}")
        except Exception as e:
            logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
            raise
    
    def _read_json_file(self, file_path: str) -> List[Dict[str, Any]]:
        """JSON íŒŒì¼ ì½ê¸°"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            return data
        elif isinstance(data, dict):
            return [data]
        else:
            raise ValueError(f"ì˜ˆìƒì¹˜ ëª»í•œ JSON êµ¬ì¡°: {type(data)}")
    
    def _read_csv_file(self, file_path: str) -> List[Dict[str, Any]]:
        """CSV íŒŒì¼ ì½ê¸°"""
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                data.append(row)
        return data
    
    def _read_text_file(self, file_path: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° (ê°„ë‹¨í•œ íŒŒì‹±)"""
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line and not line.startswith('#'):
                    # ê°„ë‹¨í•œ key=value í˜•ì‹ íŒŒì‹±
                    parts = line.split('=', 1)
                    if len(parts) == 2:
                        data.append({
                            'id': f"line_{line_num}",
                            'key': parts[0].strip(),
                            'value': parts[1].strip(),
                            'line_number': line_num
                        })
        return data
    
    def _validate_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° ê²€ì¦"""
        validated_data = []
        
        for record in data:
            try:
                if self._is_valid_record(record):
                    validated_data.append(record)
                else:
                    logger.warning(f"âš ï¸  ìœ íš¨í•˜ì§€ ì•Šì€ ë ˆì½”ë“œ ê±´ë„ˆëœ€: {record}")
            except Exception as e:
                logger.error(f"ë ˆì½”ë“œ ê²€ì¦ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f" ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(validated_data)}/{len(data)} ë ˆì½”ë“œ ìœ íš¨")
        return validated_data
    
    def _is_valid_record(self, record: Dict[str, Any]) -> bool:
        """ê°œë³„ ë ˆì½”ë“œ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            for field in self.validation_rules['required_fields']:
                if field not in record or record[field] is None:
                    return False
            
            # í•„ë“œ íƒ€ì… í™•ì¸
            for field, expected_type in self.validation_rules['field_types'].items():
                if field in record and record[field] is not None:
                    if not isinstance(record[field], expected_type):
                        return False
            
            # í•„ë“œ ì œì•½ ì¡°ê±´ í™•ì¸
            for field, constraints in self.validation_rules['field_constraints'].items():
                if field in record and record[field] is not None:
                    if not self._check_field_constraints(record[field], constraints):
                        return False
            
            # ì»¤ìŠ¤í…€ ê²€ì¦ê¸° ì‹¤í–‰
            for field, validator in self.validation_rules['custom_validators'].items():
                if field in record and record[field] is not None:
                    if not validator(record[field]):
                        return False
            
            return True
            
        except Exception as e:
            logger.error(f"ë ˆì½”ë“œ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _check_field_constraints(self, value: Any, constraints: Dict[str, Any]) -> bool:
        """í•„ë“œ ì œì•½ ì¡°ê±´ í™•ì¸"""
        try:
            if 'min_length' in constraints and isinstance(value, str):
                if len(value) < constraints['min_length']:
                    return False
            
            if 'max_length' in constraints and isinstance(value, str):
                if len(value) > constraints['max_length']:
                    return False
            
            if 'min' in constraints and isinstance(value, (int, float)):
                if value < constraints['min']:
                    return False
            
            if 'max' in constraints and isinstance(value, (int, float)):
                if value > constraints['max']:
                    return False
            
            if 'allowed_values' in constraints:
                if value not in constraints['allowed_values']:
                    return False
            
            return True
            
        except Exception:
            return False
    
    def _validate_timestamp(self, timestamp: str) -> bool:
        """íƒ€ì„ìŠ¤íƒ¬í”„ ê²€ì¦"""
        try:
            datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            return True
        except Exception:
            return False
    
    def _validate_email(self, email: str) -> bool:
        """ì´ë©”ì¼ ê²€ì¦"""
        pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        return bool(re.match(pattern, email))
    
    def _validate_phone(self, phone: str) -> bool:
        """ì „í™”ë²ˆí˜¸ ê²€ì¦"""
        pattern = r'^\+?[\d\s\-\(\)]{10,}$'
        return bool(re.match(pattern, phone))
    
    def _transform_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° ë³€í™˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        transformed_data = data.copy()
        
        for transform_func in self.transformation_pipeline:
            try:
                transformed_data = transform_func(transformed_data)
            except Exception as e:
                logger.error(f"ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨ {transform_func.__name__}: {e}")
        
        return transformed_data
    
    def _normalize_strings(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¬¸ìì—´ ì •ê·œí™”"""
        for record in data:
            for key, value in record.items():
                if isinstance(value, str):
                    record[key] = value.strip().lower()
        return data
    
    def _convert_data_types(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° íƒ€ì… ë³€í™˜"""
        for record in data:
            for key, value in record.items():
                try:
                    if key == 'value' and isinstance(value, str):
                        if '.' in value:
                            record[key] = float(value)
                        else:
                            record[key] = int(value)
                    elif key == 'timestamp' and isinstance(value, str):
                        record[key] = datetime.fromisoformat(value.replace('Z', '+00:00'))
                except Exception:
                    continue
        return data
    
    def _add_derived_fields(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """íŒŒìƒ í•„ë“œ ì¶”ê°€"""
        for record in data:
            # í•´ì‹œ ID ìƒì„±
            if 'id' not in record:
                record['id'] = hashlib.md5(str(record).encode()).hexdigest()
            
            # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
            record['processed_at'] = datetime.now().isoformat()
            
            # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì¶”ê°€
            if 'category' in record:
                record['category_group'] = self._get_category_group(record['category'])
        
        return data
    
    def _get_category_group(self, category: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ê²°ì •"""
        if category in ['A', 'B']:
            return 'high_priority'
        elif category in ['C']:
            return 'medium_priority'
        else:
            return 'low_priority'
    
    def _clean_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° ì •ë¦¬"""
        cleaned_data = []
        
        for record in data:
            # ë¹ˆ ê°’ì´ ë„ˆë¬´ ë§ì€ ë ˆì½”ë“œ ì œê±°
            empty_fields = sum(1 for v in record.values() if v is None or v == '')
            if empty_fields < len(record) * 0.5:  # 50% ì´ìƒì´ ë¹„ì–´ìˆìœ¼ë©´ ì œê±°
                cleaned_data.append(record)
        
        return cleaned_data
    
    def _enrich_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° í’ë¶€í™”"""
        if not data:
            return data
        
        # í†µê³„ ì •ë³´ ê³„ì‚°
        numeric_values = [r.get('value', 0) for r in data if isinstance(r.get('value'), (int, float))]
        
        if numeric_values:
            stats = {
                'mean': statistics.mean(numeric_values),
                'median': statistics.median(numeric_values),
                'std': statistics.stdev(numeric_values) if len(numeric_values) > 1 else 0,
                'min': min(numeric_values),
                'max': max(numeric_values)
            }
            
            # ê° ë ˆì½”ë“œì— í†µê³„ ì •ë³´ ì¶”ê°€
            for record in data:
                record['stats'] = stats
        
        return data
    
    def _update_cache(self, file_path: str, data: List[Dict[str, Any]]):
        """ìºì‹œ ì—…ë°ì´íŠ¸"""
        cache_key = hashlib.md5(file_path.encode()).hexdigest()
        self.data_cache[cache_key] = {
            'data': data,
            'timestamp': datetime.now(),
            'file_path': file_path
        }
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        with self._lock:
            stats = self.processing_stats.copy()
            stats['current_time'] = datetime.now().isoformat()
            stats['uptime'] = (datetime.now() - stats['start_time']).total_seconds()
            stats['success_rate'] = stats['successful'] / max(stats['total_processed'], 1)
            stats['error_count'] = len(self.error_log)
        
        return stats
    
    def get_data_summary(self) -> Dict[str, Any]:
        """ë°ì´í„° ìš”ì•½ ì •ë³´"""
        if not self.processed_data:
            return {'message': 'ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤'}
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        categories = Counter(r.get('category', 'unknown') for r in self.processed_data)
        
        # ê°’ ë²”ìœ„ ë¶„ì„
        values = [r.get('value', 0) for r in self.processed_data if isinstance(r.get('value'), (int, float))]
        
        summary = {
            'total_records': len(self.processed_data),
            'categories': dict(categories),
            'value_stats': {
                'count': len(values),
                'mean': statistics.mean(values) if values else 0,
                'min': min(values) if values else 0,
                'max': max(values) if values else 0
            },
            'last_processed': self.processing_stats['last_processed'].isoformat() if self.processing_stats['last_processed'] else None
        }
        
        return summary
    
    def export_processed_data(self, output_file: str, format: str = 'json') -> bool:
        """ì²˜ë¦¬ëœ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
        try:
            if format == 'json':
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(self.processed_data, f, ensure_ascii=False, indent=2, default=str)
            elif format == 'csv':
                if self.processed_data:
                    fieldnames = self.processed_data[0].keys()
                    with open(output_file, 'w', newline='', encoding='utf-8') as f:
                        writer = csv.DictWriter(f, fieldnames=fieldnames)
                        writer.writeheader()
                        writer.writerows(self.processed_data)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ í˜•ì‹: {format}")
            
            logger.info(f"ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        self.data_cache.clear()
        logger.info("ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        with self._lock:
            self.processing_stats = {
                'total_processed': 0,
                'successful': 0,
                'failed': 0,
                'start_time': datetime.now(),
                'last_processed': None
            }
        logger.info("í†µê³„ ì´ˆê¸°í™” ì™„ë£Œ") 